{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from skopt import BayesSearchCV\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from reed import *\n",
    "from cinspect import dependence, importance\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# set global notebook options\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-selection",
   "metadata": {},
   "source": [
    "## Response Model\n",
    "\n",
    "How well can we predict outcomes $Y$ conditional on treatment $T$ and other covariates $Z$?\n",
    "   - fit ML models on kitchen sink, Anna's set & basic set\n",
    "   - fit basic LR on basic set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d89348",
   "metadata": {},
   "source": [
    "### Treatent variables\n",
    "\n",
    "\n",
    "   - **redhllt**, \n",
    "   - **redllt** \n",
    "   - **refllt** \n",
    "   - **reduhl**\tCompleted re-education based on highest level of attainment\n",
    "   - **redudl**\tCompleted re-education based on detailed qualifications\n",
    "   - **redufl**\tCompleted re-education using highest lvl and detailed qualifications.\n",
    "\n",
    "### Outcome variables\n",
    "   - Mental health in 2019 (**mh**). This is the transformed mental health scores from the aggregation of mental health items of the SF-36 Health Survey, as reported by the individual in 2019. It ranges from 0 to 100, with higher scores indicating better mental health.  \n",
    "   - Working hours in 2019 (**wkhr**) records the total number of hours the individual works in all jobs in a week on average. Working hours are set to 0 for those not working. \n",
    "   - Hourly Wages in 2019 (**rlwage**) records the average hourly wage for the individualâ€™s main job in 2019. Hourly wages are set to 0 for those not working and set to missing for those reporting working more than 100 hours a week. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-wonder",
   "metadata": {},
   "source": [
    "#### Columns explicitly excluded\n",
    "   - **xwaveid** (unique identifier)\n",
    "   - **p_rcom*** (timing of completion of re-education, proxies treatment) TODO think about how we would include this\n",
    "   - **p_cotrl** (first avail 2003)\n",
    "   - **p_rdf*** (first avail 2012)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ['^reduhl$', '^rehllt$', '^redudl$', '^redufl$', '^redllt$', '^refllt$']\n",
    "outcomes = ['^rlwage$', '^mh$', '^mhbm$', '^wkhr$']\n",
    "other = [\n",
    "            '^p_rcom',\n",
    "            '^p_rdf',\n",
    "            '^p_cotrl',\n",
    "            '^xwaveid$',\n",
    "            'p_rcom18'  # ?\n",
    "            '^aedcq',  # indicate studying at start - these people should already have been removed\n",
    "            '^abnfsty',\n",
    "            '^aedcqfpt',\n",
    "            '^aedqstdy'\n",
    "]\n",
    "exclude = treatments + outcomes + other\n",
    "\n",
    "\n",
    "outcome = 'rlwage'\n",
    "treatment = 'redudl'\n",
    "optimisation_metric = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3df3ba",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, basic, df, raw = load_all_data()\n",
    "for d in [basic, df, raw]:\n",
    "    drop_missing_treatment_or_outcome(d, treatment, outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-harmony",
   "metadata": {},
   "source": [
    "### Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def construct_models():\n",
    "    models = [\n",
    "        Model('ridge',Ridge(), \n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-1,4,30)\n",
    "              }\n",
    "        ),\n",
    "        Model('lasso',Lasso(),\n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-2,4,30)\n",
    "              }\n",
    "        ), \n",
    "        Model('gbr',GradientBoostingRegressor(n_iter_no_change=20, max_depth=2),\n",
    "              parameters = {\n",
    "                'max_features':[10,20,40,60,80],\n",
    "                'learning_rate':np.logspace(-3,0,10),\n",
    "                'min_samples_leaf':np.logspace(0,3,10).astype(int)\n",
    "              }\n",
    "        ),\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b3c69",
   "metadata": {},
   "source": [
    "### Fit models and visualise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf38d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = metrics = ['r2','neg_mean_squared_error']\n",
    "\n",
    "transform = Pipeline([\n",
    "    ('impute_missing', SimpleImputer()),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "data = raw\n",
    "\n",
    "control, treated = treatment_control_split(data, treatment)\n",
    "features = regex_select(data.columns, exclude, exclude=True)\n",
    "X0,y0 = split_and_transform(control, features, outcome, transform)\n",
    "X1,y1 = split_and_transform(treated, features, outcome, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425741e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the full dataset (remove ordering by treatment in case of any order dependance in fit)\n",
    "X = np.vstack((X0,X1))\n",
    "y = np.concatenate((y0,y1))\n",
    "indx = np.arange(len(y))\n",
    "np.random.shuffle(indx)\n",
    "X = X[indx,:]\n",
    "y = y[indx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8cef6",
   "metadata": {},
   "source": [
    "### (Nested) cross-validate to evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_val(load_from_cache=False, cache_name = \"nested_cv_results.pkl\"):\n",
    "    if load_from_cache:\n",
    "        with open(cache_name,'rb') as f:\n",
    "            models, results = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        models = construct_models()\n",
    "        results = {}\n",
    "        for model in models:\n",
    "            print(f\"Fitting {model.name} ...\",end='')\n",
    "            results0 = model.nested_cv_fit_evaluate(X0,y0,optimisation_metric,evaluation_metrics)\n",
    "            results1 = model.nested_cv_fit_evaluate(X1,y1,optimisation_metric,evaluation_metrics)\n",
    "            results[model.name] = (results0, results1)\n",
    "            print(\"Done\")\n",
    "        \n",
    "        print(f\"Caching results to {cache_name}\")\n",
    "        with open(cache_name,'wb') as f:\n",
    "            pickle.dump((models,results),f)\n",
    "            \n",
    "    return models,results\n",
    " \n",
    "\n",
    "models, results = nested_cross_val(load_from_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46e035",
   "metadata": {},
   "source": [
    "#### Visualise and report model performance\n",
    "\n",
    "  - Mean and Std of prediction performance for each model (both treatment & control surface)\n",
    "  - Mean and Std of average treatment effect for each model\n",
    "  - Features responsible for treatment effect heterogeneity & functional form (with uncertainty)\n",
    "      - coefficeints for linear models\n",
    "      - partial dependence curves for non-linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid = 1000 # threshold to avoid displaying results that failed to converge entirely\n",
    "\n",
    "rows = []\n",
    "index = []\n",
    "for model_name, r in results.items():\n",
    "    tau = estimate_causal_effect(X, r[0]['estimator'],r[1]['estimator'])\n",
    "    row = {'ACE':tau.mean(),'ACE_std':tau.std()}\n",
    "    for m in evaluation_metrics:\n",
    "        key = f'test_{m}'\n",
    "        for name, result in zip(('control','treated'),r):\n",
    "            label=f\"{name}_{m}\"\n",
    "            label_std=f\"{label}_std\"\n",
    "            row[label]= result[key].mean()\n",
    "            row[label_std] = result[key].std()\n",
    "    rows.append(row)\n",
    "    index.append(model_name)\n",
    "metrics = pd.DataFrame(rows,index=index)\n",
    "metrics[metrics.abs()> invalid] = np.nan\n",
    "\n",
    "with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "    display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea70f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(15,5),sharey=True)\n",
    "ax[0].bar(metrics.index, metrics['control_r2'], yerr=metrics['control_r2_std'], align='center', alpha=0.5, capsize=10)\n",
    "ax[1].bar(metrics.index, metrics['treated_r2'], yerr=metrics['treated_r2_std'], align='center', alpha=0.5,capsize=10)\n",
    "ax[0].set_ylabel('$R^2$')\n",
    "ax[0].set_title('control model')\n",
    "ax[1].set_title('treated model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121297e",
   "metadata": {},
   "source": [
    "### Bootstraped cross-validation to estimate parameter uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463704cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_params(estimator):\n",
    "    return estimator.coef_\n",
    "\n",
    "def bootstrapped_cross_val(load_from_cache=False, cache_name=\"bootstrap_cv_results.pkl\", samples=10):\n",
    "    if load_from_cache:\n",
    "        with open(cache_name, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        models = construct_models()\n",
    "        results = {}\n",
    "        start = time.time()\n",
    "        for model in models:\n",
    "            print(f\"Fitting {model.name} ...\",end='')\n",
    "            results0 = model.bootstrap_cv_evaluate(X0,y0,optimisation_metric,extract_params,\n",
    "                                                   bootstrap_samples=samples,return_estimator=True)\n",
    "            results1 = model.bootstrap_cv_evaluate(X1,y1,optimisation_metric,extract_params,\n",
    "                                                   bootstrap_samples=samples,return_estimator=True)\n",
    "            results[model.name] = (results0, results1)\n",
    "            print(\"Done\")\n",
    "        total = time.time()-start\n",
    "        print(f\"Total time:{total} seconds\")\n",
    "        print(f\"Caching results to: {cache_name}\")\n",
    "        with open(cache_name,'wb') as f:\n",
    "            pickle.dump(results,f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "bootstrap_results = bootstrapped_cross_val(load_from_cache=False,samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb863b9",
   "metadata": {},
   "source": [
    "### Bootstrapped uncertainty on average treatment effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_causal_effect(X, models0, models1):\n",
    "    tau = []\n",
    "    for e0, e1 in zip(models0,models1):\n",
    "        y0 = e0.predict(X)\n",
    "        y1 = e1.predict(X)\n",
    "        tau.append(y1-y0)\n",
    "    \n",
    "    # array of shape len(modelsi),len(X)\n",
    "    cate = np.array(tau) \n",
    "    \n",
    "    # array of shape len(modelsi) with the ate estimate for each sample\n",
    "    ate = np.mean(cate,axis=1) \n",
    "    return ate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3b021",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (results0, results1) in bootstrap_results.items():\n",
    "    models0 = [r['estimator'] for r in results0]\n",
    "    models1 = [r['estimator'] for r in results1]\n",
    "    ate = estimate_causal_effect(X,models0, models1)\n",
    "    print(model_name, ate.mean(),ate.std()/np.sqrt(len(ate)-1))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c857b1b4",
   "metadata": {},
   "source": [
    "### Visualise distribution of hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf138c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def hyperparam_distributions(samples) -> {str:[]}:\n",
    "    \"\"\"Returns a dict from hyper-parameter name to the best values for that hyper-parameter over the samples.\"\"\"\n",
    "    distributions = defaultdict(list)\n",
    "    for sample in samples:\n",
    "        h = sample['estimator'].best_params_\n",
    "        for key, value in h.items():\n",
    "            distributions[key].append(value)\n",
    "    return distributions\n",
    "\n",
    "def plot_hyperparam_distributions(samples, title) -> None:\n",
    "    distributions = hyperparam_distributions(samples)\n",
    "    k = len(distributions)\n",
    "    fig, axes = plt.subplots(1,k,figsize=(k*5,4))\n",
    "    if k == 1:\n",
    "        axes = [axes]\n",
    "    for i, (key, values) in enumerate(distributions.items()):\n",
    "        ax = axes[i]\n",
    "        ax.hist(values)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(key)\n",
    "        ax.set_ylabel('count')\n",
    "    return fig,axes\n",
    "\n",
    "for model, (results0, results1) in bootstrap_results.items():\n",
    "    plot_hyperparam_distributions(results0,f\"{model}-control\")\n",
    "    plot_hyperparam_distributions(results1,f\"{model}-treated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, could also think about visualising distribution of coefficeints for linear models. \n",
    "# TODO, why is this results so different to what I am getting from T-learners in econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e782bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000698b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
