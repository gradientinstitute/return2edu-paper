{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from skopt import BayesSearchCV\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from reed import *\n",
    "from cinspect import dependence, importance\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "\n",
    "# set global notebook options\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sklearn.__version__\n",
    "\n",
    "def drop_missing_treatment_or_outcome(df, treatment, outcome):\n",
    "    \"\"\"\n",
    "    Drop rows missing treatment or outcome variable inplace.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Index of dropped rows.\n",
    "    \"\"\"\n",
    "    l0 = len(df)\n",
    "    missing_treatment = df.loc[df[treatment].isnull()].index\n",
    "    missing_outcome = df.loc[df[outcome].isnull()].index\n",
    "    drop = missing_treatment.union(missing_outcome)\n",
    "    df.drop(index = drop, inplace=True)\n",
    "    print(f\"Dropped {l0-len(df)} rows missing treatment or outcome.\")\n",
    "    return drop\n",
    "\n",
    "def treatment_control_split(df, treatment):\n",
    "    \"\"\"\n",
    "    Seperate control and test indices\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    control: pd.DataFrame\n",
    "        subset of rows where treatment == 0\n",
    "        \n",
    "    treated: pd.DataFrame\n",
    "        subset of rows where treatment == 1\n",
    "    \"\"\"\n",
    "    control = df[df[treatment]==0]\n",
    "    treated = df[df[treatment]==1]\n",
    "    print(f\"Treated:{len(treated)}, Control:{len(control)}\")\n",
    "    return control, treated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-excellence",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d89348",
   "metadata": {},
   "source": [
    "### Treatent variables\n",
    "\n",
    "\n",
    "   - **redhllt**, \n",
    "   - **redllt** \n",
    "   - **refllt** \n",
    "   - **reduhl**\tCompleted re-education based on highest level of attainment\n",
    "   - **redudl**\tCompleted re-education based on detailed qualifications\n",
    "   - **redufl**\tCompleted re-education using highest lvl and detailed qualifications.\n",
    "\n",
    "### Outcome variables\n",
    "   - Mental health in 2019 (**mh**). This is the transformed mental health scores from the aggregation of mental health items of the SF-36 Health Survey, as reported by the individual in 2019. It ranges from 0 to 100, with higher scores indicating better mental health.  \n",
    "   - Working hours in 2019 (**wkhr**) records the total number of hours the individual works in all jobs in a week on average. Working hours are set to 0 for those not working. \n",
    "   - Hourly Wages in 2019 (**rlwage**) records the average hourly wage for the individualâ€™s main job in 2019. Hourly wages are set to 0 for those not working and set to missing for those reporting working more than 100 hours a week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ['^reduhl$', '^rehllt$', '^redudl$', '^redufl$', '^redllt$', '^refllt$']\n",
    "outcomes = ['^rlwage$', '^mh$', '^mhbm$', '^wkhr$']\n",
    "other = [\n",
    "            '^p_rcom',\n",
    "            '^p_rdf',\n",
    "            '^p_cotrl',\n",
    "            '^xwaveid$',\n",
    "            'p_rcom18'  # ?\n",
    "            '^aedcq',  # indicate studying at start - these people should already have been removed\n",
    "            '^abnfsty',\n",
    "            '^aedcqfpt',\n",
    "            '^aedqstdy'\n",
    "]\n",
    "exclude = treatments + outcomes + other\n",
    "\n",
    "\n",
    "outcome = 'rlwage'\n",
    "treatment = 'redudl'\n",
    "optimisation_metric = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dfc6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, basic, df, raw = load_all_data()\n",
    "for d in [basic, df raw]:\n",
    "    drop_missing_treatment_or_outcome(d, treatment, outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00946b",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-selection",
   "metadata": {},
   "source": [
    "## Response Model\n",
    "\n",
    "How well can we predict outcomes $Y$ conditional on treatment $T$ and other covariates $Z$?\n",
    "   - fit ML models on kitchen sink, Anna's set & basic set\n",
    "   - fit basic LR on basic set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-wonder",
   "metadata": {},
   "source": [
    "#### Columns explicitly excluded\n",
    "   - **xwaveid** (unique identifier)\n",
    "   - **p_rcom*** (timing of completion of re-education, proxies treatment) TODO think about how we would include this\n",
    "   - **p_cotrl** (first avail 2003)\n",
    "   - **p_rdf*** (first avail 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-harmony",
   "metadata": {},
   "source": [
    "### Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def construct_models():\n",
    "    models = [\n",
    "        Model('ridge',Ridge(), \n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-1,4,30)\n",
    "              }\n",
    "        ),\n",
    "        Model('lasso',Lasso(),\n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-2,4,30)\n",
    "              }\n",
    "        ), \n",
    "        Model('gbr',GradientBoostingRegressor(n_iter_no_change=20, max_depth=2),\n",
    "              parameters = {\n",
    "                'max_features':[10,20,40,60,80],\n",
    "                'learning_rate':np.logspace(-6,0,10),\n",
    "                'min_samples_leaf':np.logspace(0,3,10).astype(int)\n",
    "              }\n",
    "        ),\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b3c69",
   "metadata": {},
   "source": [
    "### Fit models and visualise performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_transform(data, features, outcome, pipeline):\n",
    "    X = data[features]\n",
    "    n,m = X.shape\n",
    "    y = data[outcome]\n",
    "    X = transform.fit_transform(X)\n",
    "    assert X.shape == (n,m), f\"Transform changed data dimensions: {(n,m)} -> {X.shape}\"\n",
    "    return X,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c22c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = metrics = ['r2','neg_mean_squared_error']\n",
    "\n",
    "transform = Pipeline([\n",
    "    ('impute_missing', SimpleImputer()),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "\n",
    "data = raw\n",
    "\n",
    "control, treated = treatment_control_split(data, treatment)\n",
    "features = regex_select(data.columns, exclude, exclude=True)\n",
    "X0,y0 = split_and_transform(control, features, outcome, transform)\n",
    "X1,y1 = split_and_transform(treated, features, outcome, transform)\n",
    "\n",
    "models = construct_models()\n",
    "results = {}\n",
    "for model in models:\n",
    "    print(f\"Fitting {model.name} ...\",end='')\n",
    "    results0 = model.nested_cv_fit_evaluate(X0,y0,optimisation_metric,evaluation_metrics)\n",
    "    results1 = model.nested_cv_fit_evaluate(X1,y1,optimisation_metric,evaluation_metrics)\n",
    "    results[model.name] = (results0, results1)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('nested_cv_results.pkl','wb') as f:\n",
    "    pickle.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(estimator):\n",
    "    return estimator.coef_\n",
    "\n",
    "import time\n",
    "\n",
    "models = construct_models()\n",
    "results = {}\n",
    "start = time.time()\n",
    "for model in models:\n",
    "    print(f\"Fitting {model.name} ...\",end='')\n",
    "    results0 = model.bootstrap_cv_evaluate(X0,y0,optimisation_metric,extract_params,bootstrap_samples=10,return_estimator=True)\n",
    "    results1 = model.bootstrap_cv_evaluate(X1,y1,optimisation_metric,extract_params,bootstrap_samples=10,return_estimator=True)\n",
    "    results[model.name] = (results0, results1)\n",
    "    print(\"Done\")\n",
    "total = time.time()-start\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff27442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('bootstrap_cv_results.pkl','wb') as f:\n",
    "    pickle.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "est = [r['estimator'] for r in results['gbr'][0]]\n",
    "alpha = [e.best_params_ for e in est]\n",
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaae37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in np.logspace(-1,3,20):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can look at the distribution of hyper-parameters, as well as the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc5a1a",
   "metadata": {},
   "source": [
    "## Visualise and Report results\n",
    "\n",
    "  - Mean and Std of prediction performance for each model (both treatment & control surface)\n",
    "  - Mean and Std of average treatment effect for each model\n",
    "  - Features responsible for treatment effect heterogeneity & functional form (with uncertainty)\n",
    "      - coefficeints for linear models\n",
    "      - partial dependence curves for non-linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_causal_effect(X, models0, models1):\n",
    "    tau = []\n",
    "    for e0, e1 in zip(models0,models1):\n",
    "        y0 = e0.predict(X)\n",
    "        y1 = e1.predict(X)\n",
    "        tau.append(y1-y0)\n",
    "    tau = np.array(tau).mean(axis=1)\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758cc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid = 1000 # threshold to avoid displaying results that failed to converge entirely\n",
    "\n",
    "X = np.vstack((X0,X1))\n",
    "\n",
    "rows = []\n",
    "index = []\n",
    "for model_name, r in results.items():\n",
    "    tau = estimate_causal_effect(X, r[0]['estimator'],r[1]['estimator'])\n",
    "    row = {'ACE':tau.mean(),'ACE_std':tau.std()}\n",
    "    for m in evaluation_metrics:\n",
    "        key = f'test_{m}'\n",
    "        for name, result in zip(('control','treated'),r):\n",
    "            label=f\"{name}_{m}\"\n",
    "            label_std=f\"{label}_std\"\n",
    "            row[label]= result[key].mean()\n",
    "            row[label_std] = result[key].std()\n",
    "    rows.append(row)\n",
    "    index.append(model_name)\n",
    "metrics = pd.DataFrame(rows,index=index)\n",
    "metrics[metrics.abs()> invalid] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa253523",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.float_format', '{:,.2f}'.format):\n",
    "    display(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd59f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X,y,coef = make_regression(n_samples=100, n_features=20, n_informative=10, coef=True)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bootstrap import bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_param(estimator):\n",
    "    return estimator.coef_[1]#{\"first\":estimator.coef_[1], \"second\":estimator.coef_[2]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = bootstrap(model, X,y, extract_param,samples=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb87e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72553d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
