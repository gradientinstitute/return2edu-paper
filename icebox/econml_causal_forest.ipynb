{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"https://ictd2016.files.wordpress.com/2016/04/microsoft-research-logo-copy.jpg\" style=\"width 30px;\" />\n",
    "             </td>\n",
    "        <td>\n",
    "            <img src=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/MSR-ALICE-HeaderGraphic-1920x720_1-800x550.jpg\" style=\"width 100px;\"/></td>\n",
    "        </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal Random Forest and Causal Forest: Use Cases and Examples\n",
    "\n",
    "Causal Forests and Generalized Random Forests are a flexible method for estimating treatment effect heterogeneity with Random Forests. Orthogonal Random Forest (ORF) combines orthogonalization, a technique that effectively removes the confounding effect in two-stage estimation, with generalized random forests. Due to the orthogonalization aspect of this method, the ORF performs especially well in the presence of high-dimensional confounders. For more details, see [this paper](https://arxiv.org/abs/1806.03467) or the [EconML docummentation](https://econml.azurewebsites.net/).\n",
    "\n",
    "The EconML SDK implements the following OrthoForest variants:\n",
    "\n",
    "* DMLOrthoForest: suitable for continuous or discrete treatments\n",
    "\n",
    "* DROrthoForest: suitable for discrete treatments\n",
    "\n",
    "* CausalForest: suitable for both discrete and continuous treatments\n",
    "\n",
    "In this notebook, we show the performance of the ORF on synthetic and observational data. \n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "1. [Example Usage with Continuous Treatment Synthetic Data](#1.-Example-Usage-with-Continuous-Treatment-Synthetic-Data)\n",
    "2. [Example Usage with Binary Treatment Synthetic Data](#2.-Example-Usage-with-Binary-Treatment-Synthetic-Data)\n",
    "3. [Example Usage with Multiple Treatment Synthetic Data](#3.-Example-Usage-with-Multiple-Treatment-Synthetic-Data)\n",
    "4. [Example Usage with Real Continuous Treatment Observational Data](#4.-Example-Usage-with-Real-Continuous-Treatment-Observational-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main imports\n",
    "from econml.orf import DMLOrthoForest, DROrthoForest\n",
    "from econml.dml import CausalForestDML\n",
    "from econml.sklearn_extensions.linear_model import WeightedLassoCVWrapper, WeightedLasso, WeightedLassoCV\n",
    "\n",
    "# Helper imports\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.linear_model import Lasso, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Example Usage with Continuous Treatment Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 DGP \n",
    "We use the data generating process (DGP) from [here](https://arxiv.org/abs/1806.03467). The DGP is described by the following equations:\n",
    "\n",
    "\\begin{align}\n",
    "T =& \\langle W, \\beta\\rangle + \\eta, & \\;\\eta \\sim \\text{Uniform}(-1, 1)\\\\\n",
    "Y =& T\\cdot \\theta(X) + \\langle W, \\gamma\\rangle + \\epsilon, &\\; \\epsilon \\sim \\text{Uniform}(-1, 1)\\\\\n",
    "W \\sim& \\text{Normal}(0,\\, I_{n_w})\\\\\n",
    "X \\sim& \\text{Uniform}(0,1)^{n_x}\n",
    "\\end{align}\n",
    "\n",
    "where $W$ is a matrix of high-dimensional confounders and $\\beta, \\gamma$ have high sparsity.\n",
    "\n",
    "For this DGP, \n",
    "\\begin{align}\n",
    "\\theta(x) = \\exp(2\\cdot x_1).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treatment effect function\n",
    "def exp_te(x):\n",
    "    return np.exp(2*x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP constants\n",
    "np.random.seed(123)\n",
    "n = 1000\n",
    "n_w = 30\n",
    "support_size = 5\n",
    "n_x = 1\n",
    "# Outcome support\n",
    "support_Y = np.random.choice(range(n_w), size=support_size, replace=False)\n",
    "coefs_Y = np.random.uniform(0, 1, size=support_size)\n",
    "epsilon_sample = lambda n: np.random.uniform(-1, 1, size=n)\n",
    "# Treatment support \n",
    "support_T = support_Y\n",
    "coefs_T = np.random.uniform(0, 1, size=support_size)\n",
    "eta_sample = lambda n: np.random.uniform(-1, 1, size=n) \n",
    "\n",
    "# Generate controls, covariates, treatments and outcomes\n",
    "#W = np.random.normal(0, 1, size=(n, n_w))\n",
    "#X = np.random.normal(1,1,size=(n,n_x))#np.random.uniform(0, 1, size=(n, n_x))\n",
    "# Heterogeneous treatment effects\n",
    "TE = np.array([exp_te(x_i) for x_i in X])\n",
    "#T = np.dot(W[:, support_T], coefs_T) + eta_sample(n)\n",
    "#Y = TE * T + np.dot(W[:, support_Y], coefs_Y) + epsilon_sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datagen import DGPGraph\n",
    "class CF_DGP():\n",
    "    \n",
    "    def __init__(self, nx=1, n_w=30,support_size=5):\n",
    "        self.coefs_T = np.zeros(n_w)\n",
    "        self.coefs_T[0:support_size] = np.random.normal(1,1,size=support_size)\n",
    "        \n",
    "        self.coefs_Y = np.zeros(n_w)\n",
    "        self.coefs_Y[0:support_size] = np.random.uniform(0,1,size=support_size)\n",
    "\n",
    "        def fW(n):\n",
    "            n_w = 30\n",
    "            return np.random.normal(0,1,size=(n,n_w))\n",
    "\n",
    "        def fX(n):\n",
    "            n_x = 1\n",
    "            #return np.random.normal(0,1,size=(n,n_x))\n",
    "            return np.random.uniform(-1,1,size=(n,n_x))\n",
    "\n",
    "        def fT(W,n):\n",
    "            return W@self.coefs_T + np.random.uniform(-1,1,size=n) \n",
    "            \n",
    "        def fY(X,W,T,n):\n",
    "            TE = np.exp(2*X[:,0])\n",
    "            return TE*T + W@self.coefs_Y + np.random.uniform(-1,1,size=n)\n",
    "    \n",
    "        dgp = DGPGraph()\n",
    "        dgp.add_node('W',fW)\n",
    "        dgp.add_node('X',fX)\n",
    "        dgp.add_node('T',fT, parents=['W'])\n",
    "        dgp.add_node('Y',fY, parents=['X','W','T'])\n",
    "        self.dgp = dgp\n",
    "    \n",
    "d1 = CF_DGP()  \n",
    "data = d1.dgp.sample(1000)\n",
    "X, W, T, Y = data['X'], data['W'], data['T'], data['Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Train Estimator\n",
    "\n",
    "**Note:** The models in the final stage of the estimation (``model_T_final``, ``model_Y_final``) need to support sample weighting. \n",
    "\n",
    "If the models of choice do not support sample weights (e.g. ``sklearn.linear_model.LassoCV``), the ``econml`` packages provides a convenient wrapper for these models ``WeightedModelWrapper`` in order to allow sample weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORF parameters and test data\n",
    "\n",
    "X_test = np.array(list(product(np.arange(0, 1, 0.01), repeat=n_x)))\n",
    "\n",
    "subsample_ratio = 0.3\n",
    "lambda_reg = np.sqrt(np.log(n_w) / (10 * subsample_ratio * n))\n",
    "est = DMLOrthoForest(\n",
    "    n_trees=1000, min_leaf_size=5,\n",
    "    max_depth=50, subsample_ratio=subsample_ratio,\n",
    "    model_T=Lasso(alpha=lambda_reg),\n",
    "    model_Y=Lasso(alpha=lambda_reg),\n",
    "    model_T_final=WeightedLasso(alpha=lambda_reg),\n",
    "    model_Y_final=WeightedLasso(alpha=lambda_reg),\n",
    "    global_residualization=False,\n",
    "    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the built-in confidence intervals constructed via Bootstrap of Little Bags, we can specify `inference=\"blb\"` at `fit` time or leave the default `inference='auto'` which will automatically use the Bootstrap of Little Bags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(Y, T, X=X, W=W, inference=\"blb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.ate_interval(X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.ate_inference(X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate treatment effects\n",
    "treatment_effects = est.effect(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default (95%) confidence intervals for the test data\n",
    "te_lower, te_upper = est.effect_interval(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = est.effect_inference(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary_frame().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.population_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we can estimate effects and get confidence intervals and inference results using a `CausalForest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = CausalForestDML(model_t=Lasso(alpha=lambda_reg),\n",
    "                       model_y=Lasso(alpha=lambda_reg),\n",
    "                       n_estimators=4000, min_samples_leaf=5,\n",
    "                       max_depth=50,\n",
    "                       verbose=0, random_state=123)\n",
    "est2.tune(Y, T, X=X, W=W)\n",
    "est2.fit(Y, T, X=X, W=W)\n",
    "treatment_effects2 = est2.effect(X_test)\n",
    "te_lower2, te_upper2 = est2.effect_interval(X_test, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"ContinuousOrthoForest\")\n",
    "plt.plot(X_test, treatment_effects, label='ORF estimate')\n",
    "expected_te = np.array([exp_te(x_i) for x_i in X_test])\n",
    "plt.plot(X_test[:, 0], expected_te, 'b--', label='True effect')\n",
    "plt.fill_between(X_test[:, 0], te_lower, te_upper, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.title(\"CausalForest\")\n",
    "plt.plot(X_test, treatment_effects2, label='ORF estimate')\n",
    "expected_te = np.array([exp_te(x_i) for x_i in X_test])\n",
    "plt.plot(X_test[:, 0], expected_te, 'b--', label='True effect')\n",
    "plt.fill_between(X_test[:, 0], te_lower2, te_upper2, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example Usage with Binary Treatment Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. DGP \n",
    "We use the following DGP:\n",
    "\n",
    "\\begin{align}\n",
    "T \\sim & \\text{Bernoulli}\\left(f(W)\\right), &\\; f(W)=\\sigma(\\langle W, \\beta\\rangle + \\eta), \\;\\eta \\sim \\text{Uniform}(-1, 1)\\\\\n",
    "Y = & T\\cdot \\theta(X) + \\langle W, \\gamma\\rangle + \\epsilon, & \\; \\epsilon \\sim \\text{Uniform}(-1, 1)\\\\\n",
    "W \\sim & \\text{Normal}(0,\\, I_{n_w}) & \\\\\n",
    "X \\sim & \\text{Uniform}(0,\\, 1)^{n_x}\n",
    "\\end{align}\n",
    "\n",
    "where $W$ is a matrix of high-dimensional confounders, $\\beta, \\gamma$ have high sparsity and $\\sigma$ is the sigmoid function.\n",
    "\n",
    "For this DGP, \n",
    "\\begin{align}\n",
    "\\theta(x) = \\exp( 2\\cdot x_1 ).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP constants\n",
    "np.random.seed(1234)\n",
    "n = 1000\n",
    "n_w = 30\n",
    "support_size = 5\n",
    "n_x = 1\n",
    "# Outcome support\n",
    "support_Y = np.random.choice(range(n_w), size=support_size, replace=False)\n",
    "coefs_Y = np.random.uniform(0, 1, size=support_size)\n",
    "epsilon_sample = lambda n: np.random.uniform(-1, 1, size=n)\n",
    "# Treatment support\n",
    "support_T = support_Y\n",
    "coefs_T = np.random.uniform(0, 1, size=support_size)\n",
    "eta_sample = lambda n: np.random.uniform(-1, 1, size=n) \n",
    "\n",
    "# Generate controls, covariates, treatments and outcomes\n",
    "#W = np.random.normal(0, 1, size=(n, n_w))\n",
    "#X = np.random.uniform(0, 1, size=(n, n_x))\n",
    "# Heterogeneous treatment effects\n",
    "TE = np.array([exp_te(x_i) for x_i in X])\n",
    "# Define treatment\n",
    "log_odds = np.dot(W[:, support_T], coefs_T) + eta_sample(n)\n",
    "T_sigmoid = 1/(1 + np.exp(-log_odds))\n",
    "#T = np.array([np.random.binomial(1, p) for p in T_sigmoid])\n",
    "# Define the outcome\n",
    "#Y = TE * T + np.dot(W[:, support_Y], coefs_Y) + epsilon_sample(n)\n",
    "\n",
    "# ORF parameters and test data\n",
    "subsample_ratio = 0.4\n",
    "X_test = np.array(list(product(np.arange(0, 1, 0.01), repeat=n_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_CF_DGP():\n",
    "    \n",
    "    def __init__(self, n_w=30,support_size=5, beta_tx = 0, beta_yx = 0):\n",
    "        self.n_w = n_w\n",
    "        self.beta_tx = beta_tx\n",
    "        self.beta_yx = beta_yx\n",
    "        self.coefs_T = np.zeros(n_w)\n",
    "        self.coefs_T[0:support_size] = np.random.uniform(0,1,size=support_size)\n",
    "        \n",
    "        self.coefs_Y = np.zeros(n_w)\n",
    "        self.coefs_Y[0:support_size] = np.random.uniform(0,1,size=support_size)\n",
    "\n",
    "        def fW(n):\n",
    "            return np.random.normal(0,1,size=(n,self.n_w))\n",
    "\n",
    "        def fX(n):\n",
    "            return np.random.uniform(0,1,size=(n))\n",
    "\n",
    "        def fT(X,W,n):\n",
    "            log_odds = self.beta_tx*X + W@self.coefs_T + np.random.uniform(-1,1,size=n) \n",
    "            p = 1/(1 + np.exp(-log_odds))\n",
    "            return np.random.binomial(1,p)\n",
    "            \n",
    "        def fY(X,W,T,n):\n",
    "            TE = X#np.exp(2*X)\n",
    "            return TE*T +self.beta_yx*X+ W@self.coefs_Y + np.random.uniform(-1,1,size=n)\n",
    "    \n",
    "        dgp = DGPGraph()\n",
    "        dgp.add_node('W',fW)\n",
    "        dgp.add_node('X',fX)\n",
    "        dgp.add_node('T',fT, parents=['X','W'])\n",
    "        dgp.add_node('Y',fY, parents=['X','W','T'])\n",
    "        self.dgp = dgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_CF_DGP():\n",
    "    \n",
    "    def __init__(self, n_w=30,support_size=5, beta_tx = 0, beta_yx = 0):\n",
    "        self.n_w = n_w\n",
    "        self.beta_tx = beta_tx\n",
    "        self.beta_yx = beta_yx\n",
    "        self.coefs_T = np.zeros(n_w)\n",
    "        self.coefs_T[0:support_size] = np.random.uniform(0,1,size=support_size)\n",
    "        \n",
    "        self.coefs_Y = np.zeros(n_w)\n",
    "        self.coefs_Y[0:support_size] = np.random.uniform(0,1,size=support_size)\n",
    "\n",
    "        def fW(n):\n",
    "            return np.random.normal(0,1,size=(n,self.n_w))\n",
    "\n",
    "        def fX(n):\n",
    "            return np.random.uniform(0,1,size=(n))\n",
    "\n",
    "        def fT(X,W,n):\n",
    "            log_odds = self.beta_tx*X + 0.1*W@self.coefs_T + np.random.uniform(-1,1,size=n) \n",
    "            p = 1/(1 + np.exp(-log_odds))\n",
    "            return np.random.binomial(1,p)\n",
    "            \n",
    "        def fY(X,W,T,n):\n",
    "            TE = X# np.exp(2*X)\n",
    "            return TE*T +self.beta_yx*X+ W@self.coefs_Y + np.random.uniform(-1,1,size=n)\n",
    "    \n",
    "        dgp = DGPGraph()\n",
    "        dgp.add_node('W',fW)\n",
    "        dgp.add_node('X',fX)\n",
    "        dgp.add_node('T',fT, parents=['X','W'])\n",
    "        dgp.add_node('Y',fY, parents=['X','W','T'])\n",
    "        self.dgp = dgp\n",
    "        \n",
    "d2 = Binary_CF_DGP(beta_tx = 1, beta_yx = 1)\n",
    "data = d2.dgp.sample(2000)\n",
    "X, W, T, Y = data['X'].reshape(-1,1), data['W'], data['T'], data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# propensity of T|X,W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Train Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = DROrthoForest(\n",
    "    n_trees=200, min_leaf_size=10,\n",
    "    max_depth=30, subsample_ratio=subsample_ratio,\n",
    "    propensity_model = LogisticRegression(C=1/(X.shape[0]*lambda_reg), penalty='l1', solver='saga'),\n",
    "    model_Y = Lasso(alpha=lambda_reg),\n",
    "    propensity_model_final=LogisticRegression(C=1/(X.shape[0]*lambda_reg), penalty='l1', solver='saga'), \n",
    "    model_Y_final=WeightedLasso(alpha=lambda_reg)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(Y, T, X=X, W=W)\n",
    "dro_ate = est.ate(X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate treatment effects for the default treatment points T0=0 and T1=1\n",
    "treatment_effects = est.effect(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default (95%) confidence intervals for the default treatment points T0=0 and T1=1\n",
    "te_lower, te_upper = est.effect_interval(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = CausalForestDML(model_y=Lasso(alpha=lambda_reg),\n",
    "                       model_t=LogisticRegression(C=1/(X.shape[0]*lambda_reg)),\n",
    "                       n_estimators=200, min_samples_leaf=5,\n",
    "                       max_depth=50, max_samples=subsample_ratio/2,\n",
    "                       discrete_treatment=True,\n",
    "                       random_state=123)\n",
    "est2.fit(Y, T, X=X, W=W, cache_values=True)\n",
    "treatment_effects2 = est2.effect(X_test)\n",
    "te_lower2, te_upper2 = est2.effect_interval(X_test)\n",
    "cf_ate = est2.at(X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#est2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "models = GradientBoostingRegressor()\n",
    "est3 = TLearner(models=models)\n",
    "est_t.fit(Y, T, X=X)\n",
    "est_t.ate(X=X), est_t.ate_interval(X=X)\n",
    "\n",
    "models = \n",
    "est3 = TLearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_te = d2.dgp.cate(20000,'T','Y','X',X_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"DiscreteTreatmentOrthoForest\")\n",
    "plt.plot(X_test, treatment_effects, label='ORF estimate')\n",
    "expected_te = np.array([exp_te(x_i) for x_i in X_test])\n",
    "#plt.plot(X_test[:, 0], expected_te, 'b--', label='True effect')\n",
    "plt.plot(X_test[:,0], estimated_te,'r--',label='MCMC True effect')\n",
    "plt.fill_between(X_test[:, 0], te_lower, te_upper, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"CausalForest\")\n",
    "plt.plot(X_test, treatment_effects2, label='ORF estimate')\n",
    "expected_te = np.array([exp_te(x_i) for x_i in X_test])\n",
    "#plt.plot(X_test[:, 0], expected_te, 'b--', label='True effect')\n",
    "plt.fill_between(X_test[:, 0], te_lower2, te_upper2, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example Usage with Multiple Treatment Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. DGP \n",
    "We use the following DGP:\n",
    "\n",
    "\\begin{align}\n",
    "Y = & \\sum_{t=1}^{n_{\\text{treatments}}} 1\\{T=t\\}\\cdot \\theta_{T}(X) + \\langle W, \\gamma\\rangle + \\epsilon, \\; \\epsilon \\sim \\text{Unif}(-1, 1), \\\\\n",
    "\\text{Pr}[T=t \\mid W] \\propto & \\exp\\{\\langle W, \\beta_t \\rangle\\}, \\;\\;\\;\\; \\forall t\\in \\{0, 1, \\ldots, n_{\\text{treatments}}\\} \n",
    "\\end{align}\n",
    "\n",
    "where $W$ is a matrix of high-dimensional confounders, $\\beta_t, \\gamma$ are sparse.\n",
    "\n",
    "For this particular example DGP we used $n_{\\text{treatments}}=3$ and \n",
    "\\begin{align}\n",
    "\\theta_1(x) = & \\exp( 2 x_1 ),\\\\\n",
    "\\theta_2(x) = &  3 \\cdot \\sigma(100\\cdot (x_1 - .5)),\\\\\n",
    "\\theta_3(x) = & -2 \\cdot \\sigma(100\\cdot (x_1 - .25)),\n",
    "\\end{align}\n",
    "where $\\sigma$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_data(n, n_w, support_size, n_x, te_func, n_treatments):\n",
    "    # Outcome support\n",
    "    support_Y = np.random.choice(range(n_w), size=support_size, replace=False)\n",
    "    coefs_Y = np.random.uniform(0, 1, size=support_size)\n",
    "    epsilon_sample = lambda n: np.random.uniform(-1, 1, size=n)\n",
    "    # Treatment support \n",
    "    support_T = support_Y\n",
    "    coefs_T = np.random.uniform(0, 1, size=(support_size, n_treatments))\n",
    "    eta_sample = lambda n: np.random.uniform(-1, 1, size=n) \n",
    "    # Generate controls, covariates, treatments and outcomes\n",
    "    W = np.random.normal(0, 1, size=(n, n_w))\n",
    "    X = np.random.uniform(0, 1, size=(n, n_x))\n",
    "    # Heterogeneous treatment effects\n",
    "    TE = np.array([te_func(x_i, n_treatments) for x_i in X])\n",
    "    log_odds = np.dot(W[:, support_T], coefs_T)\n",
    "    T_sigmoid = np.exp(log_odds)\n",
    "    T_sigmoid = T_sigmoid/np.sum(T_sigmoid, axis=1, keepdims=True)\n",
    "    T = np.array([np.random.choice(n_treatments, p=p) for p in T_sigmoid])\n",
    "    TE = np.concatenate((np.zeros((n,1)), TE), axis=1)\n",
    "    Y = TE[np.arange(n), T] + np.dot(W[:, support_Y], coefs_Y) + epsilon_sample(n)\n",
    "    X_test = np.array(list(product(np.arange(0, 1, 0.01), repeat=n_x)))\n",
    "\n",
    "    return (Y, T, X, W), (X_test, np.array([te_func(x, n_treatments) for x in X_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "def te_func(x, n_treatments):\n",
    "    return [np.exp(2*x[0]), 3*scipy.special.expit(100*(x[0] - .5)) - 1, -2*scipy.special.expit(100*(x[0] - .25))]\n",
    "\n",
    "np.random.seed(123)\n",
    "(Y, T, X, W), (X_test, te_test) = get_test_train_data(2000, 3, 3, 1, te_func, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Train Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = DROrthoForest(n_trees=500, model_Y = WeightedLasso(alpha=lambda_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est.fit(Y, T, X=X, W=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate marginal treatment effects\n",
    "treatment_effects = est.const_marginal_effect(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default (95%) marginal confidence intervals for the test data\n",
    "te_lower, te_upper = est.const_marginal_effect_interval(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = est.const_marginal_effect_inference(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = CausalForestDML(model_y=Lasso(alpha=lambda_reg),\n",
    "                       model_t=LogisticRegression(C=1/(X.shape[0]*lambda_reg)),\n",
    "                       n_estimators=4000, min_samples_leaf=5,\n",
    "                       max_depth=50, max_samples=subsample_ratio/2,\n",
    "                       discrete_treatment=True,\n",
    "                       random_state=123)\n",
    "est2.fit(Y, T, X=X, W=W)\n",
    "treatment_effects2 = est2.const_marginal_effect(X_test)\n",
    "te_lower2, te_upper2 = est2.const_marginal_effect_interval(X_test, alpha=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"DiscreteTreatmentOrthoForest\")\n",
    "y = treatment_effects\n",
    "colors = ['b', 'r', 'g']\n",
    "for it in range(y.shape[1]):\n",
    "    plt.plot(X_test[:, 0], te_test[:, it], '--', label='True effect T={}'.format(it), color=colors[it])\n",
    "    plt.fill_between(X_test[:, 0], te_lower[:, it], te_upper[:, it], alpha=0.3, color='C{}'.format(it))\n",
    "    plt.plot(X_test, y[:, it], label='ORF estimate T={}'.format(it), color='C{}'.format(it))\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"CausalForest\")\n",
    "y = treatment_effects2\n",
    "colors = ['b', 'r', 'g']\n",
    "for it in range(y.shape[1]):\n",
    "    plt.plot(X_test[:, 0], te_test[:, it], '--', label='True effect T={}'.format(it), color=colors[it])\n",
    "    plt.fill_between(X_test[:, 0], te_lower2[:, it], te_upper2[:, it], alpha=0.3, color='C{}'.format(it))\n",
    "    plt.plot(X_test, y[:, it], label='ORF estimate T={}'.format(it), color='C{}'.format(it))\n",
    "plt.ylabel(\"Treatment Effect\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Example Usage with Real Continuous Treatment Observational Data\n",
    "\n",
    "We applied our technique to Dominick’s dataset, a popular historical dataset of store-level orange juice prices and sales provided by University of Chicago Booth School of Business. \n",
    "\n",
    "The dataset is comprised of a large number of covariates $W$, but researchers might only be interested in learning the elasticity of demand as a function of a few variables $x$ such\n",
    "as income or education. \n",
    "\n",
    "We applied the `ContinuousTreatmentOrthoForest` to estimate orange juice price elasticity\n",
    "as a function of income, and our results, unveil the natural phenomenon that lower income consumers are more price-sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few more imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "file_name = \"oj_large.csv\"\n",
    "\n",
    "if not os.path.isfile(file_name):\n",
    "    print(\"Downloading file (this might take a few seconds)...\")\n",
    "    urllib.request.urlretrieve(\"https://msalicedatapublic.blob.core.windows.net/datasets/OrangeJuice/oj_large.csv\", file_name)\n",
    "oj_data = pd.read_csv(file_name)\n",
    "oj_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "Y = oj_data['logmove'].values\n",
    "T = np.log(oj_data[\"price\"]).values\n",
    "scaler = StandardScaler()\n",
    "W1 = scaler.fit_transform(oj_data[[c for c in oj_data.columns if c not in ['price', 'logmove', 'brand', 'week', 'store']]].values)\n",
    "W2 = pd.get_dummies(oj_data[['brand']]).values\n",
    "W = np.concatenate([W1, W2], axis=1)\n",
    "X = oj_data[['INCOME']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Train Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "n_trees = 1000\n",
    "min_leaf_size = 50\n",
    "max_depth = 20\n",
    "subsample_ratio = 0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = DMLOrthoForest(\n",
    "        n_trees=n_trees, min_leaf_size=min_leaf_size, max_depth=max_depth, \n",
    "        subsample_ratio=subsample_ratio,\n",
    "        model_T=Lasso(alpha=0.1),\n",
    "        model_Y=Lasso(alpha=0.1),\n",
    "        model_T_final=WeightedLassoCVWrapper(cv=3), \n",
    "        model_Y_final=WeightedLassoCVWrapper(cv=3)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "est.fit(Y, T, X=X, W=W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_income = 10.0 \n",
    "max_income = 11.1\n",
    "delta = (max_income - min_income) / 100\n",
    "X_test = np.arange(min_income, max_income + delta - 0.001, delta).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate marginal treatment effects\n",
    "treatment_effects = est.const_marginal_effect(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate default (95%) marginal confidence intervals for the test data\n",
    "te_upper, te_lower = est.const_marginal_effect_interval(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2 = CausalForestDML(model_y=WeightedLassoCVWrapper(cv=3),\n",
    "                       model_t=WeightedLassoCVWrapper(cv=3),\n",
    "                       n_estimators=n_trees, min_samples_leaf=min_leaf_size, max_depth=max_depth,\n",
    "                       max_samples=subsample_ratio/2,\n",
    "                       random_state=123)\n",
    "est2.fit(Y, T, X=X, W=W)\n",
    "treatment_effects2 = est2.effect(X_test)\n",
    "te_lower2, te_upper2 = est2.effect_interval(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Orange Juice elasticity as a function of income\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(X_test.flatten(), treatment_effects, label=\"OJ Elasticity\")\n",
    "plt.fill_between(X_test.flatten(), te_lower, te_upper, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.xlabel(r'$\\log$(Income)')\n",
    "plt.ylabel('Orange Juice Elasticity')\n",
    "plt.legend()\n",
    "plt.title(\"Orange Juice Elasticity vs Income: ContinuousTreatmentOrthoForest\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(X_test.flatten(), treatment_effects2, label=\"OJ Elasticity\")\n",
    "plt.fill_between(X_test.flatten(), te_lower2, te_upper2, label=\"95% BLB CI\", alpha=0.3)\n",
    "plt.xlabel(r'$\\log$(Income)')\n",
    "plt.ylabel('Orange Juice Elasticity')\n",
    "plt.legend()\n",
    "plt.title(\"Orange Juice Elasticity vs Income: CausalForest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e5c6628eef985e7fd2fa2aad22c988c5b8aa1d2648cf9c51c543a2a2637c546"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
