{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from skopt import BayesSearchCV\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from reed import *\n",
    "from cinspect import dependence, importance\n",
    "\n",
    "# set global notebook options\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-excellence",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d89348",
   "metadata": {},
   "source": [
    "### Treatent variables\n",
    "\n",
    "\n",
    "   - **redhllt**, ?\n",
    "   - **redllt** ?\n",
    "   - **refllt** ?\n",
    "   - **reduhl**\tCompleted re-education based on highest level of attainment\n",
    "   - **redudl**\tCompleted re-education based on detailed qualifications\n",
    "   - **redufl**\tCompleted re-education using highest lvl and detailed qualifications.\n",
    "\n",
    "### Outcome variables\n",
    "   - Mental health in 2019 (**mh**). This is the transformed mental health scores from the aggregation of mental health items of the SF-36 Health Survey, as reported by the individual in 2019. It ranges from 0 to 100, with higher scores indicating better mental health.  \n",
    "   - Working hours in 2019 (**wkhr**) records the total number of hours the individual works in all jobs in a week on average. Working hours are set to 0 for those not working. \n",
    "   - Hourly Wages in 2019 (**rlwage**) records the average hourly wage for the individualâ€™s main job in 2019. Hourly wages are set to 0 for those not working and set to missing for those reporting working more than 100 hours a week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ['reduhl', 'rehllt', 'redudl', 'redufl', 'redllt', 'refllt']\n",
    "outcomes = ['rlwage', 'mh', 'mhbm', 'wkhr']\n",
    "outcome = 'rlwage'\n",
    "treatment = 'redudl'\n",
    "optimisation_metric = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b5280",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0126d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, basic, df, raw = load_all_data()\n",
    "train_indx, test_indx, train_indx0, test_indx0, train_indx1, test_indx1 = drop_missing_and_split(\n",
    "    [basic,df,raw],\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    test_size=0 \n",
    ")\n",
    "\n",
    "features = select_features(df,treatments,outcomes,outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _, y_train, _,t_train,_,_ = prepare_data(df,features,outcome,treatment,train_indx,test_indx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf8272",
   "metadata": {},
   "source": [
    "## Causal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0cb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.dml import LinearDML, SparseLinearDML\n",
    "model = LinearDML(mc_iters=10).fit(\n",
    "    Y = y_train,\n",
    "    T = t_train,\n",
    "    W = X_train,\n",
    "    #X = X_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f3bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ate_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.metalearners import XLearner\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "xm = XLearner(models = Ridge(),propensity_model=LogisticRegression(max_iter=1000))\n",
    "xm.fit(Y=y_train,T=t_train,X=X_train,inference='bootstrap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xm.ate_interval(X=X_train,T0=0,T1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d08ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from econml.metalearners import SLearner,TLearner\n",
    "tm = TLearner(models=Ridge())\n",
    "tm.fit(Y=y_train,T=t_train, X=X_train,inference='bootstrap')\n",
    "tm.ate_interval(X=X_train,T0=0,T1=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SLearner(overall_model=Ridge())\n",
    "sm.fit(Y=y_train,T=t_train,X=X_train,inference='bootstrap')\n",
    "sm.ate_interval(X=X_train,T0=0,T1=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-selection",
   "metadata": {},
   "source": [
    "## Response Model\n",
    "\n",
    "How well can we predict outcomes $Y$ conditional on treatment $T$ and other covariates $Z$?\n",
    "   - fit ML models on kitchen sink, Anna's set & basic set\n",
    "   - fit basic LR on basic set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998fe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatments = ['reduhl', 'rehllt', 'redudl', 'redufl', 'redllt', 'refllt']\n",
    "outcomes = ['rlwage', 'mh', 'mhbm', 'wkhr']\n",
    "outcome = 'rlwage'\n",
    "treatment = 'redudl'\n",
    "optimisation_metric = 'neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "meta, basic, df, raw = load_all_data()\n",
    "\n",
    "train_indx, test_indx, train_indx0, test_indx0, train_indx1, test_indx1 = drop_missing_and_split(\n",
    "    [basic,df,raw],\n",
    "    outcome=outcome,\n",
    "    treatment=treatment,\n",
    "    test_size=.33 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c336e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naughty-wonder",
   "metadata": {},
   "source": [
    "#### Columns explicitly excluded\n",
    "   - **xwaveid** (unique identifier)\n",
    "   - **p_rcom*** (timing of completion of re-education, proxies treatment) TODO think about how we would include this\n",
    "   - **p_cotrl** (first avail 2003)\n",
    "   - **p_rdf*** (first avail 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-harmony",
   "metadata": {},
   "source": [
    "### Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def construct_models():\n",
    "    models = [\n",
    "        Model('linear-regression',LinearRegression()),\n",
    "        Model('ridge',Ridge(), \n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-1,3,20)\n",
    "              }\n",
    "        ),\n",
    "        Model('lasso',Lasso(),\n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-2,5,40)\n",
    "              }\n",
    "        ), \n",
    "        Model('gbr',GradientBoostingRegressor(n_iter_no_change=20, max_depth=2),\n",
    "              parameters = {\n",
    "                'max_features':[10,20,40,60,80],\n",
    "                'learning_rate':np.logspace(-6,-1,10),\n",
    "                'min_samples_leaf':np.logspace(0,3,10).astype(int)\n",
    "              }\n",
    "        ),\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b3c69",
   "metadata": {},
   "source": [
    "### Fit models and visualise performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108982b2",
   "metadata": {},
   "source": [
    "#### Large feature set\n",
    "\n",
    "This is a set of features selected by Anna as the broad set that may be relevant. Some variables have been one-hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b1bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_l = select_features(df,treatments,outcomes,outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl_train0, Xl_test0, yl_train0, yl_test0, t_train0, t_test0,_ = prepare_data(df,features_l,outcome,treatment,train_indx0,test_indx0)\n",
    "Xl_train1, Xl_test1, yl_train1, yl_test1, t_train1,t_test1,_ = prepare_data(df,features_l,outcome,treatment,train_indx1,test_indx1)\n",
    "Xl = np.vstack((Xl_train0,Xl_train1,Xl_test0,Xl_test1))\n",
    "yl = np.concatenate((yl_train0,yl_train1, yl_test0,yl_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e69667",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_l0 = construct_models()\n",
    "fit_models(models_l0,optimisation_metric,Xl_train0,yl_train0)\n",
    "models_l1 = construct_models()\n",
    "fit_models(models_l1,optimisation_metric,Xl_train1,yl_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e1dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_regression_performance(models_l0,Xl_test0,yl_test0)\n",
    "visualise_regression_performance(models_l1,Xl_test1,yl_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edd55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_l0[1].fit_estimator.best_estimator_\n",
    "coef = pd.DataFrame({\"feature\":features_l,\"coef\":model.coef_})\n",
    "coef.sort_values('coef',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165225ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_indx = features_l.index(\"p_wh01\")\n",
    "f_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dependence.individual_conditional_expectation(model, Xl_train1, 117,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10a826",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xl_train0[:,111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e557d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xl_train0.min(),Xl_train0.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdba367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cinspect.dependence import PartialDependencePlot\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22feb895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if categorical, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = PartialDependencePlot(\"pd\")\n",
    "pdp.add_dependence(model, Xl_train0, 111, 'hours worked',density='hist')\n",
    "pdp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a0bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = Xl_train0[:,117]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb27b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(values, np.array([.01,.99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b6c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(values) # only two values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38871509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Xl_train0[:,117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538deadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid, predictions, color, name = pdp.curves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3506bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we may want to compute coefficients and partial dependence etc with respect to variables pre-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e81faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "total = len(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.choice(np.arange(total),size=n_samples,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931db886",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "ax.plot(grid,predictions.mean(axis=0),color='black')\n",
    "ax.plot(grid,predictions[sample,:].T, color='black',alpha=0.1,lw=1);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a4e428",
   "metadata": {},
   "outputs": [],
   "source": [
    " if color is None:\n",
    "        color = \"black\"\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    if sample is not None:\n",
    "        ax.plot(grid_values, predictions[sample].T, alpha=0.1, color=\"grey\")\n",
    "        if density:\n",
    "            for x in X[sample, feature_indx]:\n",
    "                ax.axvline(x, color=\"grey\", ymin=0, ymax=0.03, alpha=0.2)\n",
    "\n",
    "    ax.plot(grid_values, predictions.mean(axis=0), color=color, label=label)\n",
    "\n",
    "    ax.set_ylabel(\"prediction\")\n",
    "    ax.set_xlabel(feature_name)\n",
    "    if title is None:\n",
    "        ax.set_title(\"ICE & Partial Dependence for {}\".format(feature_name))\n",
    "    else:\n",
    "        ax.set_title(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef14f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to write a version of feature importance that looks just at how much the outcome changes with respect to input\n",
    "\n",
    "class TLearner:\n",
    "    def __init__(self,name,model0,model1):\n",
    "        self.model0 = model0\n",
    "        self.model1 = model1\n",
    "        self.name = name\n",
    "        \n",
    "    def y0(self,X):\n",
    "        return self.model0.fit_estimator.predict(X)\n",
    "    \n",
    "    def y1(self,X):\n",
    "        return self.model1.fit_estimator.predict(X)\n",
    "    \n",
    "    def tau(self,X):\n",
    "        return self.y1(X) - self.y0(X)\n",
    "\n",
    "    def ate(self,X):\n",
    "        tau = self.tau(X)\n",
    "        return np.mean(tau)\n",
    "    \n",
    "\n",
    "def visualise_causal_estimation(models0,models1,X):\n",
    "    estimators = {}\n",
    "    for model0,model1 in zip(models0,models1):\n",
    "        causal_estimator = TLearner(model0.name,model0,model1)\n",
    "        estimators[model0.name] = causal_estimator\n",
    "        ate = causal_estimator.ate(X)\n",
    "        print(f\"{causal_estimator.name}:ATE={ate:.2f}\")\n",
    "        y0,y1 = causal_estimator.y0(X),causal_estimator.y1(X)\n",
    "        fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "        ax[0].set_title(causal_estimator.name)\n",
    "        ax[0].scatter(y0,y1,alpha=0.1)\n",
    "        ax[0].set_xlabel('y0')\n",
    "        ax[0].set_ylabel('y1')\n",
    "        ax[1].hist(y0,alpha=0.5,label=\"y0\")\n",
    "        ax[1].hist(y1,alpha=0.5,label=\"y1\")\n",
    "        ax[1].legend(loc=\"upper left\") \n",
    "    return estimators\n",
    "        \n",
    "# feature importance ...\n",
    "# how much does changing X change tau?\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def permutation_importance(X,func,metric,repeat=5):\n",
    "    \"\"\"Compute the extent to which the function depends on each column of X.\"\"\"\n",
    "    change = []\n",
    "    y = np.tile(func(X),repeat)\n",
    "    columns = np.arange(X.shape[1])\n",
    "    for c in columns:\n",
    "        X0 = X.copy()\n",
    "        yp = []\n",
    "        for r in range(repeat):\n",
    "            np.random.shuffle(X0[:,c])\n",
    "            yp.append(func(X0))\n",
    "        yp = np.concatenate(yp)\n",
    "        dy = metric(y,yp)\n",
    "        change.append(dy)\n",
    "    return change\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36b0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_estimators = visualise_causal_estimation(models_l0,models_l1,Xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f517d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yl_train1.mean() - yl_train0.mean() # unadjusted, hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise individual or partial dependence curves (for the difference and for each regression model side seperately)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7665d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007f6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance? on tau. \n",
    "pi = permutation_importance(Xl, causal_estimators['gbr'].tau, mean_squared_error)\n",
    "fi = pd.DataFrame({'feature':features_l,'importance':pi}).sort_values(by='importance',ascending=False)\n",
    "fi.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in fi.head(10)['feature']:\n",
    "    print(k, meta.column_names_to_labels.get(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc94a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we filtered to those who got re-educated in 2002-2017 (in current data, those who completed in 2018)\n",
    "## dummy variable for those who got re-educated in 2018 or 2019 (drop those people)\n",
    "## 25th November presentation deadline, returning to education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a0ee0",
   "metadata": {},
   "source": [
    "#### Basic feature set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cf41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic0 = basic[basic[treatment]==0]\n",
    "basic1 = basic[basic[treatment]==1]\n",
    "features_b = select_features(basic,treatments,outcomes,outcome)\n",
    "Xb_train0, Xb_test0, yb_train0, yb_test0, t_train0,t_test0,tr0 = prepare_data(basic0,features_b,outcome,treatment,train_indx0,test_indx0)\n",
    "Xb_train1, Xb_test1, yb_train1, yb_test1, t_train1,t_test1,tr0 = prepare_data(basic1,features_b,outcome,treatment,train_indx1,test_indx1)\n",
    "Xb = np.vstack((Xb_train0,Xb_train1,Xb_test0,Xb_test1))\n",
    "yb = np.concatenate((yb_train0,yb_train1, yb_test0,yb_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84effad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_b0 = construct_models()\n",
    "fit_models(models_b0,optimisation_metric,Xb_train0,yb_train0)\n",
    "models_b1 = construct_models()\n",
    "fit_models(models_b1,optimisation_metric,Xb_train1,yb_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff76468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_regression_performance(models_b0,Xb_test0,yb_test0)\n",
    "visualise_regression_performance(models_b1,Xb_test1,yb_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75039509",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_causal_estimation(models_b0,models_b1,Xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = permutation_importance(Xb, causal_estimator.tau, mean_squared_error)\n",
    "pd.DataFrame({'feature':features_b,'importance':pi}).sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbf1aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a90d316",
   "metadata": {},
   "source": [
    "#### Raw feature set\n",
    "\n",
    "This feature set contains every variable observed in 2001, with very little filtering or pre-processing. The minimal preprocessing includes;\n",
    "   - removing variables that are more than 95% missing\n",
    "   - merging variables that are almost perfectly correlated (> .95) \n",
    "   - removing variables with 0 variance\n",
    "   - changing dates to days past an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f782064",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_raw = construct_models()\n",
    "treatment = 'redudl' #reduhl, #refllt\n",
    "df,meta = load_data('raw',treatments,outcomes)\n",
    "features_r = select_features(df)\n",
    "Xr_train, Xr_test, yr_train, yr_test = prepare_data(df, features_r, treatment,train_indx,test_indx)\n",
    "fit_models(models_raw,optimisation_metric,Xr_train,yr_train)\n",
    "visualise_performance(models_raw,Xr_test,yr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b87833",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_r = extract_importance(models_raw,Xr_test,yr_test,features_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_importance_distribution(importances_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e78b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_large = construct_models()\n",
    "treatment = 'redudl' #reduhl, #refllt\n",
    "df,meta_l = load_data('anna',treatments,outcomes)\n",
    "features_l = select_features(df)\n",
    "Xl_train, Xl_test, yl_train, yl_test = prepare_data(df, features_l, treatment,train_indx,test_indx)\n",
    "fit_models(models_large,optimisation_metric,Xl_train,yl_train)\n",
    "visualise_performance(models_large,Xl_test,yl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_l = extract_importance(models_large,Xl_test,yl_test,features_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938488e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_importance_distribution(importances_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290be6b",
   "metadata": {},
   "source": [
    "##### Features ranked by permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e195b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_labels = meta_l.column_names_to_labels\n",
    "importances_l['label'] = [ column_labels.get(name,\"\") for name in importances_l.index]\n",
    "importances_l.sort_values('permutation-lr-ridge',ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dfbd8d",
   "metadata": {},
   "source": [
    "#### Minimal feature set\n",
    "This is the very minimal set of features used in the original paper. It consists of 4 variables, (sex, age, education, employment). Each is one-hot encoded and interactions are added between sex and the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ba81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_basic = [construct_models()[0]]\n",
    "treatment = 'redudl' #reduhl, #refllt\n",
    "df,meta = load_data('basic',treatments,outcomes)\n",
    "features_b = select_features(df) \n",
    "Xb_train, Xb_test, yb_train, yb_test = prepare_data(df, features_b, treatment,train_indx,test_indx)\n",
    "fit_models(models_basic,optimisation_metric,Xb_train,yb_train)\n",
    "visualise_performance(models_basic,Xb_test,yb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95343ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_b = extract_importance(models_basic,Xb_test,yb_test,features_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952ec0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_importance_distribution(importances_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f24012",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importances_b.sort_values('permutation-lr',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73da44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO add permutation curve for age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfb017",
   "metadata": {},
   "source": [
    "# Causal Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39572b",
   "metadata": {},
   "source": [
    "## Direct Regression\n",
    "\n",
    "Predict the outcome $Y$ based on pre-treatment variables and the treatment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ccc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'wkhr'\n",
    "\n",
    "transform = Pipeline([\n",
    "    ('impute_missing', SimpleImputer()),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "XD = transform.fit_transform(df[features])\n",
    "yD = df[outcome]\n",
    "\n",
    "assert np.ndim(y)==1\n",
    "\n",
    "valid = ~np.isnan(yD)\n",
    "XD = XD[valid,:]\n",
    "yD = yD[valid]\n",
    "\n",
    "models = [\n",
    "    Model('gbc',GradientBoostingRegressor(n_iter_no_change=20, max_depth=2),\n",
    "          parameters = {\n",
    "            'max_features':[10,20,40,60,80],\n",
    "            'learning_rate':np.logspace(-6,-1,10),\n",
    "            'min_samples_leaf':np.logspace(0,3,10).astype(int)\n",
    "          }\n",
    "    ),\n",
    "    Model('lr',LinearRegression(),\n",
    "          parameters = {\n",
    "              'C':np.logspace(-4,0,20)\n",
    "          }\n",
    "    )\n",
    "]\n",
    "\n",
    "inner_cv = KFold(n_splits=5)\n",
    "outer_cv = KFold(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37525af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimisation_metric = 'neg_mean_squared_error'\n",
    "fit_models = []\n",
    "for model in models:\n",
    "    search = GridSearchCV(\n",
    "        estimator=model.estimator, param_grid=model.parameters, verbose=2,\n",
    "        n_jobs=-1, scoring = optimisation_metric, cv = inner_cv, refit=True\n",
    "    )\n",
    "    search.fit(XD,yD)\n",
    "    model.fit_estimator = search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e36756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add calibration curves\n",
    "for model in models:\n",
    "    p = model.fit_estimator.predict_proba(X)[:,1]\n",
    "    visualise_propensity_model_performance(y,p, model.name)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59e19f",
   "metadata": {},
   "source": [
    "# Additional code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-harvard",
   "metadata": {},
   "source": [
    "### Fit a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_features = np.sqrt(X.shape[1])\n",
    "\n",
    "gbc_params = {\n",
    "     #'min_samples_split': (1e-3, .2, 'log-uniform'),\n",
    "     'max_features': (int(sqrt_features/2),int(sqrt_features*5)),\n",
    "     'learning_rate':(0.00001,0.1,'log-uniform'),\n",
    "     'min_samples_leaf':(1,2,4,8,16,32,64,128,256,512,1024)\n",
    "}\n",
    "\n",
    "gbc_param_grid = {\n",
    "    'max_features':[10,20,40,60,80],\n",
    "    'learning_rate':np.logspace(-6,-1,10),\n",
    "    'min_samples_leaf':np.logspace(0,3,10).astype(int)\n",
    "}\n",
    "\n",
    "search = GridSearchCV(estimator=GradientBoostingClassifier(n_iter_no_change=20, max_depth=2),verbose=2,param_grid=gbc_param_grid,n_jobs=-1,scoring='roc_auc')\n",
    "\n",
    "# ncalls = 300\n",
    "# search = BayesSearchCV(\n",
    "#     estimator=GradientBoostingClassifier(n_iter_no_change=20, max_depth=2),search_spaces = gbc_params, cv=inner_cv,n_iter=ncalls,\n",
    "#     scoring = 'roc_auc',\n",
    "#     n_jobs = 10\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df86fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X,y)\n",
    "#search.fit(X,y,callback=tqdm_skopt(total=ncalls, desc=\"Searching Hyperparams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c69cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5627f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def plot_knn_surface(results,param, ax):\n",
    "\n",
    "    param_name = \"param_\"+param\n",
    "    X = results[param_name].values.reshape(-1,1)\n",
    "    y = results[\"mean_test_score\"]\n",
    "    max_neighbours = int(len(y)/10)\n",
    "    model = GridSearchCV(KNeighborsRegressor(),param_grid={\"n_neighbors\":range(2,max_neighbours)})\n",
    "    model.fit(X,y)\n",
    "    X_test = np.linspace(results[param_name].min(),results[param_name].max(),100).reshape(-1,1)\n",
    "    y_test = model.predict(X_test)\n",
    "    ax.plot(X_test.ravel(),y_test)\n",
    "\n",
    "fig, ax = plt.subplots(2,nparams,figsize=(5*nparams,12))\n",
    "for i, pname in enumerate(gbc_params.keys()):\n",
    "    values = results[f\"param_{pname}\"]\n",
    "    \n",
    "    ax[0,i].scatter(values,results['mean_test_score'],alpha=0.2)\n",
    "    ax[0,i].set_title(pname)\n",
    "    ax[0,i].set_xlabel(pname)\n",
    "    ax[0,i].set_ylabel(\"mean score\")\n",
    "    \n",
    "    if values.nunique() < 10:\n",
    "        results.groupby(f\"param_{pname}\")['mean_test_score'].mean().plot(ax=ax[1,i])\n",
    "    else:\n",
    "        plot_knn_surface(results, pname, ax[1,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56cef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def plot_knn_surface(results,param, ax):\n",
    "\n",
    "    param_name = \"param_\"+param\n",
    "    X = results[param_name].values.reshape(-1,1)\n",
    "    y = results[\"mean_test_score\"]\n",
    "    max_neighbours = int(len(y)/10)\n",
    "    model = GridSearchCV(KNeighborsRegressor(),param_grid={\"n_neighbors\":range(2,max_neighbours)})\n",
    "    model.fit(X,y)\n",
    "    X_test = np.linspace(results[param_name].min(),results[param_name].max(),100).reshape(-1,1)\n",
    "    y_test = model.predict(X_test)\n",
    "    ax.plot(X_test.ravel(),y_test)\n",
    "\n",
    "fig, ax = plt.subplots(2,nparams,figsize=(5*nparams,12))\n",
    "for i, pname in enumerate(gbc_params.keys()):\n",
    "    values = results[f\"param_{pname}\"]\n",
    "    \n",
    "    ax[0,i].scatter(values,results['mean_test_score'],alpha=0.2)\n",
    "    ax[0,i].set_title(pname)\n",
    "    ax[0,i].set_xlabel(pname)\n",
    "    ax[0,i].set_ylabel(\"mean score\")\n",
    "    \n",
    "    if values.nunique() < 10:\n",
    "        results.groupby(f\"param_{pname}\")['mean_test_score'].mean().plot(ax=ax[1,i])\n",
    "    else:\n",
    "        plot_knn_surface(results, pname, ax[1,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-communication",
   "metadata": {},
   "source": [
    "### Fit models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(lr.estimator,param_grid = lr.parameters, cv=inner_cv,scoring='roc_auc')\n",
    "scores = cross_val_score(model,X,y,cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = {}\n",
    "#for mname, model in models.items():\n",
    "#    scores[mname] = cross_val_score(model, X, y, scoring='roc_auc') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-adapter",
   "metadata": {},
   "source": [
    "### Propensity model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame({'coef':models['lr'].coef_[0]}, index = features)\n",
    "coef['abs'] = coef['coef'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in coef.sort_values('abs',ascending=False).head(50).index:\n",
    "    print(name, meta.column_names_to_labels[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-arrangement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
