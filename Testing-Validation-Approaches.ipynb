{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Comparing validation approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from reed import Model\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 500\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = 'y_Dwsce'#'y_wsce'\n",
    "treatment = 'redufl'\n",
    "optimisation_metric = 'neg_mean_squared_error'\n",
    "evaluation_metrics = ('r2','neg_mean_squared_error')\n",
    "log_outcome=False\n",
    "data_file = \"all_vars.csv\"\n",
    "cross_val_cache = None\n",
    "bootstrap_cache = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reed import drop_missing_treatment_or_outcome\n",
    "data = pd.read_csv(data_file,index_col='xwaveid')\n",
    "drop_missing_treatment_or_outcome(data, treatment, outcome)\n",
    "if log_outcome:\n",
    "    data[outcome] = np.log(data[outcome]+data[outcome].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import seperate_and_transform_data\n",
    "X0, X1, y0, y1, X, y, t, features = seperate_and_transform_data(data, treatment, outcome)\n",
    "\n",
    "print(\"Control data dimensions: \",X0.shape)\n",
    "print(\"Treated data dimensions:\",X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import importance_from_coef\n",
    "def construct_models():\n",
    "    models = [\n",
    "        Model('ridge',Ridge(), \n",
    "              parameters = {\n",
    "                  'alpha':np.logspace(-1,4,10)\n",
    "              },\n",
    "              importance_func=importance_from_coef\n",
    "        )\n",
    "    ]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import print_unconditional_effects\n",
    "print_unconditional_effects(data, treatment, y0, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(5))\n",
    "l2 = list('abjcd')\n",
    "l3 = [construct_models()[0],construct_models()[0]]\n",
    "\n",
    "np.array(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import nested_cross_val\n",
    "models0, models1, results = nested_cross_val(\n",
    "    construct_models,\n",
    "    cross_val_cache,\n",
    "    X0, X1, y0, y1,\n",
    "    optimisation_metric,\n",
    "    evaluation_metrics,\n",
    "    innercv=5,\n",
    "    outercv=10,\n",
    "    load_from_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import estimate_causal_effect\n",
    "def compute_ate(results, X, evaluation_metrics):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for model_name, (contr_result, treat_result) in results.items():\n",
    "        tau = estimate_causal_effect(X, contr_result['estimator'],treat_result['estimator'])\n",
    "        row = {'ACE':tau.mean(),'ACE_std':tau.std()}\n",
    "        \n",
    "        for m in evaluation_metrics:\n",
    "            key = f'test_{m}'\n",
    "            for name, result in [('control',contr_result),('treated',treat_result)]:\n",
    "                label=f\"{name}_{m}\"\n",
    "                label_std=f\"{label}_std\"\n",
    "                row[label]= result[key].mean()\n",
    "                row[label_std] = result[key].std()\n",
    "        rows.append(row)\n",
    "        index.append(model_name)\n",
    "    metrics = pd.DataFrame(rows,index=index)\n",
    "    return metrics\n",
    "\n",
    "compute_ate(results,X,evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, (results0, results1) in results.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results0['estimator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_distributions(samples) -> {str:[]}:\n",
    "    \"\"\"Returns a dict from hyper-parameter name to the best values for that hyper-parameter over the samples.\"\"\"\n",
    "    distributions = defaultdict(list)\n",
    "    bounds = defaultdict(lambda:[np.inf,-np.inf])\n",
    "    for sample in samples:\n",
    "        h = sample['estimator'].best_params_\n",
    "        grid = sample['estimator'].param_grid\n",
    "        for key, value in h.items():\n",
    "            distributions[key].append(value)\n",
    "            if key in grid:\n",
    "                search_space = grid[key]\n",
    "                minv, maxv = np.min(search_space),np.max(search_space)\n",
    "                if bounds[key][0] > minv:\n",
    "                    bounds[key][0] = minv\n",
    "                if bounds[key][1] < maxv:\n",
    "                    bounds[key][1] = maxv\n",
    "    return distributions,bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from direct_regression import plot_hyperparam_distributions\n",
    "for model, (results0, results1) in results.items():\n",
    "    plot_hyperparam_distributions(results0,f\"{model}-control\")\n",
    "    plot_hyperparam_distributions(results1,f\"{model}-treated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_fit_evaluate(self, X, y,\n",
    "                               optimisation_metric,\n",
    "                               evaluation_metrics,\n",
    "                               inner_cv=None,\n",
    "                               outer_cv=None\n",
    "                               ):\n",
    "        estimator = self.setup_estimator(optimisation_metric, inner_cv)\n",
    "        outer_cv = self._setup_cv(outer_cv)\n",
    "\n",
    "        nested_results = cross_validate(estimator, X=X, y=y, cv=outer_cv,\n",
    "                                        scoring=evaluation_metrics, return_estimator=True)\n",
    "        return nested_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, (contr_result, treat_result) in results.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
