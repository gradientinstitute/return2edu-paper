{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import re\n",
    "import string\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import networkx as nx\n",
    "import reed\n",
    "from clean import *\n",
    "\n",
    "\n",
    "pd.options.display.max_columns=100\n",
    "pd.options.display.max_colwidth=200\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,m,e = 'a','q','s' # select which waves to base analysis on\n",
    "min_start_age = 25 # the minimum age people must as of the starting wave\n",
    "missing_threshold = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in initial wave 19914\n"
     ]
    }
   ],
   "source": [
    "# read the combined file for the starting wave\n",
    "df1, meta1 = pyreadstat.read_sav(f'../part1/Combined {s}190c.sav') \n",
    "n0 = len(df1)\n",
    "print(f\"Number of people in initial wave {n0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatments: Index(['xwaveid', 'redudl', 'reduhl', 'redufl'], dtype='object')\n",
      "Outcomes: Index(['xwaveid', 'y_jbhruc', 'y_ghmh', 'y_wsce', 'y_wscei', 'y_employment',\n",
      "       'y_Djbhruc', 'y_Dghmh', 'y_Dwsce', 'y_Dwscei', 'y_Demployment'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from treatment_outcomes import compute_treatment_vars, compute_outcomes\n",
    "treatments = compute_treatment_vars(df1, s, m)\n",
    "outcomes = compute_outcomes(df1, s, e)\n",
    "print(\"Treatments:\",treatments.columns)\n",
    "print(\"Outcomes:\",outcomes.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10087, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_outcomes = pd.merge(treatments,outcomes,on='xwaveid',how='inner')\n",
    "treatment_outcomes['xwaveid'] = treatment_outcomes['xwaveid'].astype(int)\n",
    "treatment_outcomes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dh==0</th>\n",
       "      <th>dh==1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dl==0</th>\n",
       "      <td>7833</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dl==1</th>\n",
       "      <td>1503</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dh==0  dh==1\n",
       "dl==0   7833    701\n",
       "dl==1   1503    929"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reed import compute_confusion\n",
    "compute_confusion(treatments['redudl'],treatments['reduhl'],'dl','dh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 6125 individuals who are not present in waves q and s (50%)\n"
     ]
    }
   ],
   "source": [
    "from treatment_outcomes import simplify_employment\n",
    "from reed import regex_select\n",
    "\n",
    "def extract_basic_variables(df):\n",
    "    # age, sex, education in 2001, employment status in 2001\n",
    "    basic = df1[['xwaveid','ahgage','ahgsex','aedhigh1','aesdtl']].copy() \n",
    "\n",
    "    def simplify_education(v):\n",
    "        \"\"\"Simplify down to match categories in paper.\"\"\"\n",
    "        if v < 0 or v==10:\n",
    "            return np.nan # missing\n",
    "        if v < 3: #(above bachelors)\n",
    "            return 2\n",
    "        return v # < year 12:(9), year 12:(8), cert:(5), diploma/adv diploma:(4), bachelors/honours:(3)\n",
    "\n",
    "    # remove under 25s\n",
    "    basic = basic[basic['ahgage']>25].copy()\n",
    "    \n",
    "    # simplify education & employment in line with baseline paper\n",
    "    basic['aesdtl']=basic['aesdtl'].apply(simplify_employment)\n",
    "    basic['aedhigh1'] = basic['aedhigh1'].apply(simplify_education)\n",
    "    \n",
    "    # bin age\n",
    "    basic['ahgage'] = pd.cut(basic['ahgage'],bins=[24,34,44,54,120])\n",
    "    \n",
    "    # dummy encode\n",
    "    basic = pd.get_dummies(basic,columns=['ahgage','ahgsex','aedhigh1','aesdtl'],drop_first=True)\n",
    "    \n",
    "    # add interactions between gender and other variables\n",
    "    age_edu_emp = regex_select(basic.columns,['^ahgage_','^aedhigh1_','^aesdtl_'])\n",
    "    basic = create_interaction_columns(basic,['ahgsex_2.0'],age_edu_emp)\n",
    "    basic['xwaveid'] = basic['xwaveid'].astype(int)\n",
    "    return basic\n",
    "\n",
    "basic = extract_basic_variables(df1)\n",
    "l0 = len(basic)\n",
    "basic = pd.merge(basic,treatment_outcomes,on='xwaveid',how='inner')\n",
    "l1 = len(basic)\n",
    "print(f\"Dropped {l0-l1} individuals who are not present in waves {m} and {e} ({100*(l0-l1)/l0:.0f}%)\")\n",
    "basic.set_index('xwaveid',inplace=True)\n",
    "basic.to_csv(\"basic_variables.csv\",index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_participants(df1,min_start_age):\n",
    "    \"\"\"\n",
    "    Remove those already studying or below the minimum age in the initial wave.\n",
    "    \"\"\"\n",
    "    n0 = len(df1)\n",
    "    df = df1.loc[df1[f'{s}hgage'] >= min_start_age].copy()\n",
    "    print(f\"Dropping {n0-len(df)} participants below age {min_start_age}\")\n",
    "\n",
    "    # filter out those already studying\n",
    "\n",
    "    # If any of the following are > 0, then the respondant was already studying at the beginning of the period\n",
    "    already_studying_cols = [s+col for col in ['caeft','caept','nlreast','bncsty','bnfsty']]\n",
    "\n",
    "    already_studying = df[already_studying_cols].sum(axis=1)\n",
    "\n",
    "    n0 = len(df)\n",
    "    df = df[already_studying < 1].copy()\n",
    "    print(f\"Dropping {n0-len(df)} participants already studying at period start\")\n",
    "    print(f\"Remaining participants:{len(df)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_type_information():\n",
    "    headers = ['variable','vartype','format','label','long_label','varcat','relevance',\"0\"]\n",
    "    type_df = pd.read_csv(\"HILDAw1vardic.csv\",skiprows=4,index_col=None, names=headers)\n",
    "    type_df['relevance'] = type_df['relevance'].fillna(1).astype(int)\n",
    "    type_df.loc[type_df['label']=='ACAEPT','relevance'] = -1\n",
    "    return type_df\n",
    "\n",
    "def drop_irrelevant_columns_inplace(df, type_df):\n",
    "    irrelevant = list(type_df.loc[type_df['relevance']<1,'variable'])\n",
    "    irrelevant.remove('xwaveid')\n",
    "    df.drop(columns=irrelevant,inplace=True)\n",
    "    print(f\"Dropped {len(irrelevant)} irrelevant columns.\")\n",
    "    return irrelevant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_types_inplace(df1):\n",
    "    # Reformat some of the columns\n",
    "    dates = [\"ahhhqivw\",\"ahhcompi\",\"ahhcompf\",\"ahhcomps\",\"ahhidate\"]\n",
    "    string = ['ahhtitle']\n",
    "    categorical = [\n",
    "     'acca1',\n",
    "     'acca2',\n",
    "     'ahhmgfxd',\n",
    "     'ahhmgmxd',\n",
    "     'ahhp1',\n",
    "     'ahhp2',\n",
    "     'ahhp3',\n",
    "     'ahhpgfxd',\n",
    "     'ahhpgmxd',\n",
    "     'ahhpno',\n",
    "     'xwaveid'\n",
    "    ]\n",
    "\n",
    "    for c in categorical:\n",
    "        if c in df1.columns:\n",
    "            df1[c] = pd.to_numeric(df1[c])\n",
    "\n",
    "    # turn into days past epoch\n",
    "    basedate = pd.to_datetime('01/01/1900',format='%d/%m/%Y')    \n",
    "    for c in dates:\n",
    "        if c in df1.columns:\n",
    "            df1[c] = (pd.to_datetime(df1[c],format='%d/%m/%Y',errors='coerce')-basedate).dt.days \n",
    "\n",
    "    for c in string:\n",
    "        df1[c] = df1[c].astype('category').cat.codes\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_raw_data(min_age=25, threshold=0.99):\n",
    "    df1, meta1 = pyreadstat.read_sav(f'../part1/Combined {s}190c.sav') \n",
    "    print(\"Read in data, with shape:\",df1.shape)\n",
    "    df1 = filter_participants(df1,min_age)\n",
    "    \n",
    "    columns_dropped = {} # keep track of why each column was dropped\n",
    "    \n",
    "    type_df = read_type_information()\n",
    "    irrelevant = drop_irrelevant_columns_inplace(df1,type_df)\n",
    "    add_list_to_dict(irrelevant,columns_dropped,'invalid/irrelevant')\n",
    "\n",
    "    fix_types_inplace(df1)\n",
    "    \n",
    "    constant = drop_constant_columns(df1)\n",
    "    add_list_to_dict(constant,columns_dropped,'constant')\n",
    "\n",
    "    mostly_missing = drop_mostly_missing_columns(df1, threshold = threshold)\n",
    "    drop_strongly_correlated_columns(df1,columns_dropped,threshold=threshold,inplace=True,fillna=True)\n",
    "    \n",
    "    print(\"Processed data, with shape:\",df1.shape)\n",
    "    return df1, columns_dropped\n",
    "\n",
    "def drop_strongly_correlated_columns(df, columns_dropped,threshold=0.99, inplace=True, fillna=True):\n",
    "    \"\"\"\n",
    "    Drop columns that are highly correlated with another column.\n",
    "    \n",
    "    In each strongly connected component, the variable with the least missing data will be the \n",
    "    representative variable kept.\n",
    "    \"\"\"\n",
    "    d = df.fillna(-1)\n",
    "    c = compute_correlations(d)\n",
    "    del d\n",
    "    strong = c[c['correlation'].abs() > threshold]\n",
    "    print(f\"number of strong correlations stronger than {threshold:.2f} is: {len(strong)}\")\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from(list((strong[['c1','c2']]).itertuples(index=False,name=None)))\n",
    "    components = list(nx.connected_components(g))\n",
    "    print(\"Number of connected components in strong correlation graph:\",len(components))\n",
    "    \n",
    "    drop = set({})\n",
    "    for component in components:\n",
    "        least_missing = df[list(component)].isnull().sum(axis=0).idxmin()\n",
    "        component.remove(least_missing)\n",
    "        drop = drop.union(component)\n",
    "        add_list_to_dict(component,columns_dropped,f\"merged into {least_missing}\")\n",
    "    print(\"Columns dropped due to almost perfect correlation:\",len(drop))\n",
    "    if inplace:\n",
    "        df.drop(columns=drop,inplace=True)\n",
    "    return drop, columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in data, with shape: (19914, 3400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 7359 participants below age 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 1216 participants already studying at period start\n",
      "Remaining participants:11339\n",
      "Dropped 403 irrelevant columns.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 108 columns that are constant or entirely missing\n",
      "Dropping 1440 columns with more than 95% missing \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of strong correlations stronger than 0.95 is: 5781\n",
      "Number of connected components in strong correlation graph: 137\n",
      "Columns dropped due to almost perfect correlation: 643\n",
      "Processed data, with shape: (11339, 806)\n",
      "Dropped 5793 individuals who are not present in waves q and s (51%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written data to: all_vars_950.csv\n"
     ]
    }
   ],
   "source": [
    "df, columns_dropped = filter_raw_data(threshold=missing_threshold)\n",
    "l0 = len(df)\n",
    "df = pd.merge(df,treatment_outcomes,on='xwaveid',how='inner')\n",
    "l1 = len(df)\n",
    "print(f\"Dropped {l0-l1} individuals who are not present in waves {m} and {e} ({100*(l0-l1)/l0:.0f}%)\")\n",
    "df.set_index('xwaveid',inplace=True)\n",
    "filename = f\"all_vars_{int(round(missing_threshold*1000))}.csv\"\n",
    "df.to_csv(filename,index=True)\n",
    "print(\"Written data to:\",filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}