{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1",
   "metadata": {
    "tags": [
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyreadstat\n",
    "import re\n",
    "import string\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import networkx as nx\n",
    "import reed\n",
    "import pickle\n",
    "from clean import *\n",
    "\n",
    "\n",
    "pd.options.display.max_columns=100\n",
    "pd.options.display.max_colwidth=200\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Set which waves to base the analysis on, what the minimum age must be to be considered and above what threshold a column is excluded due to missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s,m,e = 'a','q','s' # select which waves to base analysis on\n",
    "min_start_age = 25 # the minimum age people must as of the starting wave\n",
    "missing_threshold = 0.95\n",
    "correlation_threshold = 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data Structure\n",
    "\n",
    "- Part1 contains the combined data from all questionairs asked in a given wave. Each wave is a separate file (eg a s wave 1, be is wave 2, etc. \n",
    "- TODO understand the note in HILDA manual that concepts that contained both positive and negative values were split across multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in initial wave 19914\n"
     ]
    }
   ],
   "source": [
    "# read the combined file for the starting wave\n",
    "df1, meta1 = pyreadstat.read_sav(f'../part1/Combined {s}190c.sav') \n",
    "n0 = len(df1)\n",
    "print(f\"Number of people in initial wave {n0}\")\n",
    "with open('data/metadata.pkl','wb') as f:\n",
    "    pickle.dump(meta1,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Filter people who were already studying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 7359 participants below age 25\n",
      "Dropping 1216 participants already studying at period start\n",
      "Remaining participants:11339\n"
     ]
    }
   ],
   "source": [
    "def filter_participants(df1,min_start_age):\n",
    "    \"\"\"\n",
    "    Remove those already studying or below the minimum age in the initial wave.\n",
    "    \"\"\"\n",
    "    n0 = len(df1)\n",
    "    df = df1.loc[df1[f'{s}hgage'] >= min_start_age].copy()\n",
    "    print(f\"Dropping {n0-len(df)} participants below age {min_start_age}\")\n",
    "\n",
    "    # filter out those already studying\n",
    "\n",
    "    # If any of the following are > 0, then the respondant was already studying at the beginning of the period\n",
    "    already_studying_cols = [s+col for col in ['caeft','caept','nlreast','bncsty','bnfsty']]\n",
    "\n",
    "    already_studying = df[already_studying_cols].sum(axis=1)\n",
    "\n",
    "    n0 = len(df)\n",
    "    df = df[already_studying < 1].copy()\n",
    "    print(f\"Dropping {n0-len(df)} participants already studying at period start\")\n",
    "    print(f\"Remaining participants:{len(df)}\")\n",
    "    return df\n",
    "\n",
    "df1 = filter_participants(df1,min_start_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Compute treatment & outcomes\n",
    "#### Outcomes measures\n",
    "   - **hours worked** Largly missing. The following variables are perfectly correlated\n",
    "      - `ajbhru` 57% missing, *E1b Hours per week usually worked in all your jobs*\n",
    "      - `ajbhruc`,57% missing *DV: Hours per week usually worked in all jobs*\n",
    "   - **wages (not normalised by hours worked)**\n",
    "      - `awsfe` wages from all jobs last financial year with imputation from net\n",
    "      - `awsce` current weekly wages from all jobs with imputation from net\n",
    "      - Non-imputed versions of both of these exist (replace the final `e` with `g`) but have slightly more missing values\n",
    "      - `awsfhave` records if people have received income from salary/wages last financial year. We could also use `_esbrd` to tell if people should have a non-zero wage.\n",
    "      - wage variables have quite a lot of missing data (~33% missing `awsfe` and 30% missing both `awsfe` and `awsfhave`)\n",
    "      - There are versions of wage variables with imputation of missing data from based on responses from the participant in other waves and responses from similar participants. These are indicated by the suffix `i` (eg `awsfei`, `awscei`). These variables contain no missing data.\n",
    "   - **employment status (categorical outcome)**\n",
    "   - **mental health**\n",
    "   \n",
    "#### Treatment measures\n",
    "   - Treatment is based on a change in education qualification between 2001 and 2017\n",
    "   - There are a number of study related variables that are only recorded on a subset of the waves. \n",
    "   - _edq{XXX} variables are recorded every year and count the number of qualifications a person holds in each of a number of categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatments: Index(['xwaveid', 'redudl', 'reduhl', 'redufl'], dtype='object')\n",
      "Outcomes: Index(['xwaveid', 'y_jbhruc', 'y_ghmh', 'y_wsce', 'y_wscei', 'y_employment',\n",
      "       'y_Djbhruc', 'y_Dghmh', 'y_Dwsce', 'y_Dwscei', 'y_Demployment'],\n",
      "      dtype='object')\n",
      "Updated computation of treatment\n"
     ]
    }
   ],
   "source": [
    "from treatment_outcomes import compute_treatment_vars, compute_outcomes\n",
    "treatments = compute_treatment_vars(df1, s, m)\n",
    "outcomes = compute_outcomes(df1, s, e)\n",
    "treatment_outcomes = pd.merge(treatments,outcomes,on='xwaveid',how='inner')\n",
    "treatment_outcomes['xwaveid'] = treatment_outcomes['xwaveid'].astype(int)\n",
    "print(\"Treatments:\",treatments.columns)\n",
    "print(\"Outcomes:\",outcomes.columns)\n",
    "print(\"Updated computation of treatment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Confusion matrix for highest vs count based treatment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dh==0</th>\n",
       "      <th>dh==1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dl==0</th>\n",
       "      <td>4488</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dl==1</th>\n",
       "      <td>953</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dh==0  dh==1\n",
       "dl==0   4488      3\n",
       "dl==1    953    600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reed import compute_confusion\n",
    "compute_confusion(treatments['redudl'],treatments['reduhl'],'dl','dh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Extract basic variables\n",
    "Extract a data set corresponding to the original paper we are working to extend - based on the table below;\n",
    "![image.png](images/original_paper_table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 5793 individuals who are not present in waves q and s (51%)\n"
     ]
    }
   ],
   "source": [
    "from treatment_outcomes import simplify_employment\n",
    "from reed import regex_select\n",
    "\n",
    "def extract_basic_variables(df):\n",
    "    # age, sex, education in 2001, employment status in 2001\n",
    "    basic = df1[['xwaveid','ahgage','ahgsex','aedhigh1','aesdtl']].copy() \n",
    "\n",
    "    def simplify_education(v):\n",
    "        \"\"\"Simplify down to match categories in paper.\"\"\"\n",
    "        if v < 0 or v==10:\n",
    "            return np.nan # missing\n",
    "        if v < 3: #(above bachelors)\n",
    "            return 2\n",
    "        return v # < year 12:(9), year 12:(8), cert:(5), diploma/adv diploma:(4), bachelors/honours:(3)\n",
    "    \n",
    "    # simplify education & employment in line with baseline paper\n",
    "    basic['aesdtl']=basic['aesdtl'].apply(simplify_employment)\n",
    "    basic['aedhigh1'] = basic['aedhigh1'].apply(simplify_education)\n",
    "    \n",
    "    # bin age\n",
    "    basic['ahgage'] = pd.cut(basic['ahgage'],bins=[24,34,44,54,120])\n",
    "    \n",
    "    # dummy encode\n",
    "    basic = pd.get_dummies(basic,columns=['ahgage','ahgsex','aedhigh1','aesdtl'],drop_first=True)\n",
    "    \n",
    "    # add interactions between gender and other variables\n",
    "    age_edu_emp = regex_select(basic.columns,['^ahgage_','^aedhigh1_','^aesdtl_'])\n",
    "    basic = create_interaction_columns(basic,['ahgsex_2.0'],age_edu_emp)\n",
    "    basic['xwaveid'] = basic['xwaveid'].astype(int)\n",
    "    return basic\n",
    "\n",
    "basic = extract_basic_variables(df1)\n",
    "l0 = len(basic)\n",
    "basic_with_outcomes = pd.merge(basic,treatment_outcomes,on='xwaveid',how='inner')\n",
    "l1 = len(basic_with_outcomes)\n",
    "print(f\"Dropped {l0-l1} individuals who are not present in waves {m} and {e} ({100*(l0-l1)/l0:.0f}%)\")\n",
    "basic_with_outcomes.set_index('xwaveid',inplace=True)\n",
    "basic_with_outcomes.to_csv(\"basic_variables.csv\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Extract Full variable set\n",
    "Extract a 'kitchen sink' dataset with minimal filtering of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Filter out columns based on annotated spreadsheet\n",
    "Remove columns that have been manually marked as irrelevant or proxies to whether someone is already studying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def read_type_information():\n",
    "    headers = ['variable','vartype','format','label','long_label','varcat','relevance',\"0\"]\n",
    "    type_df = pd.read_csv(\"HILDAw1vardic.csv\",skiprows=4,index_col=None, names=headers)\n",
    "    type_df['relevance'] = type_df['relevance'].fillna(1).astype(int)\n",
    "    type_df.loc[type_df['label']=='ACAEPT','relevance'] = -1\n",
    "    return type_df\n",
    "\n",
    "def drop_irrelevant_columns_inplace(df, type_df):\n",
    "    irrelevant = list(type_df.loc[type_df['relevance']<1,'variable'])\n",
    "    irrelevant.remove('xwaveid')\n",
    "    df.drop(columns=irrelevant,inplace=True)\n",
    "    print(f\"Dropped {len(irrelevant)} irrelevant columns.\")\n",
    "    return irrelevant\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Fix types\n",
    "   - encode categorical values & strings as integers (ordinal rather than one-hot)\n",
    "   - transform dates into days past 01/01/1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def fix_types_inplace(df1):\n",
    "    # Reformat some of the columns\n",
    "    dates = [\"ahhhqivw\",\"ahhcompi\",\"ahhcompf\",\"ahhcomps\",\"ahhidate\"]\n",
    "    string = ['ahhtitle']\n",
    "    categorical = [\n",
    "     'acca1',\n",
    "     'acca2',\n",
    "     'ahhmgfxd',\n",
    "     'ahhmgmxd',\n",
    "     'ahhp1',\n",
    "     'ahhp2',\n",
    "     'ahhp3',\n",
    "     'ahhpgfxd',\n",
    "     'ahhpgmxd',\n",
    "     'ahhpno',\n",
    "     'xwaveid'\n",
    "    ]\n",
    "\n",
    "    for c in categorical:\n",
    "        if c in df1.columns:\n",
    "            df1[c] = pd.to_numeric(df1[c])\n",
    "\n",
    "    # turn into days past epoch\n",
    "    basedate = pd.to_datetime('01/01/1900',format='%d/%m/%Y')    \n",
    "    for c in dates:\n",
    "        if c in df1.columns:\n",
    "            df1[c] = (pd.to_datetime(df1[c],format='%d/%m/%Y',errors='coerce')-basedate).dt.days \n",
    "\n",
    "    for c in string:\n",
    "        df1[c] = df1[c].astype('category').cat.codes\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Automatic (very basic) column filtering\n",
    "   - drop columns where the proportion of data missing is above the maximum threshold\n",
    "   - drop columns that are constant (zero variance)\n",
    "   - drop columns that are very tightly correlated (based on correlation threshold) with another column that contains less missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_raw_data(df1, missing_threshold=0.99, correlation_threshold=0.99):\n",
    "    columns_dropped = {} # keep track of why each column was dropped\n",
    "    \n",
    "    type_df = read_type_information()\n",
    "    irrelevant = drop_irrelevant_columns_inplace(df1,type_df)\n",
    "    add_list_to_dict(irrelevant,columns_dropped,'invalid/irrelevant')\n",
    "\n",
    "    fix_types_inplace(df1)\n",
    "    \n",
    "    constant = drop_constant_columns(df1)\n",
    "    add_list_to_dict(constant,columns_dropped,'constant')\n",
    "\n",
    "    mostly_missing = drop_mostly_missing_columns(df1, threshold = missing_threshold)\n",
    "    drop_strongly_correlated_columns(df1,columns_dropped,threshold=correlation_threshold,inplace=True,fillna=True)\n",
    "    \n",
    "    print(\"Processed data, with shape:\",df1.shape)\n",
    "    return df1, columns_dropped\n",
    "\n",
    "def drop_strongly_correlated_columns(df, columns_dropped,threshold=0.99, inplace=True, fillna=True):\n",
    "    \"\"\"\n",
    "    Drop columns that are highly correlated with another column.\n",
    "    \n",
    "    In each strongly connected component, the variable with the least missing data will be the \n",
    "    representative variable kept.\n",
    "    \"\"\"\n",
    "    d = df.fillna(-1)\n",
    "    c = compute_correlations(d)\n",
    "    del d\n",
    "    strong = c[c['correlation'].abs() > threshold]\n",
    "    print(f\"number of strong correlations stronger than {threshold:.2f} is: {len(strong)}\")\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from(list((strong[['c1','c2']]).itertuples(index=False,name=None)))\n",
    "    components = list(nx.connected_components(g))\n",
    "    print(\"Number of connected components in strong correlation graph:\",len(components))\n",
    "    \n",
    "    drop = set({})\n",
    "    for component in components:\n",
    "        least_missing = df[list(component)].isnull().sum(axis=0).idxmin()\n",
    "        component.remove(least_missing)\n",
    "        drop = drop.union(component)\n",
    "        add_list_to_dict(component,columns_dropped,f\"merged into {least_missing}\")\n",
    "    print(\"Columns dropped due to almost perfect correlation:\",len(drop))\n",
    "    if inplace:\n",
    "        df.drop(columns=drop,inplace=True)\n",
    "    return drop, columns_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Save the data to file\n",
    "Save the data to file for subsequent model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 403 irrelevant columns.\n",
      "Dropping 108 columns that are constant or entirely missing\n",
      "Dropping 1440 columns with more than 95% missing \n",
      "number of strong correlations stronger than 0.90 is: 9856\n",
      "Number of connected components in strong correlation graph: 164\n",
      "Columns dropped due to almost perfect correlation: 810\n",
      "Processed data, with shape: (11339, 639)\n",
      "Dropped 5793 individuals who are not present in waves q and s (51%)\n",
      "Written data to: all_vars.csv\n"
     ]
    }
   ],
   "source": [
    "df, columns_dropped = filter_raw_data(df1, missing_threshold=missing_threshold,correlation_threshold=correlation_threshold)\n",
    "l0 = len(df)\n",
    "df = pd.merge(df,treatment_outcomes,on='xwaveid',how='inner')\n",
    "l1 = len(df)\n",
    "print(f\"Dropped {l0-l1} individuals who are not present in waves {m} and {e} ({100*(l0-l1)/l0:.0f}%)\")\n",
    "df.set_index('xwaveid',inplace=True)\n",
    "filename = f\"all_vars.csv\"\n",
    "df.to_csv(filename,index=True)\n",
    "print(\"Written data to:\",filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (basic_with_outcomes.index == df.index).all(), \"index should be the same across datasets\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
